---
title: "Supervised learning aggregated"
author: "Cornelius Erfort"
date: "5/10/2021"
output: 
  pdf_document:
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, tidy.opts=list(width.cutoff = 80), tidy = T, python.reticulate = F)

```


# Setting up

This script requires the files "sample_germany.dta" and "data_joint.RDS" in the parent directory. It also writes and reads two csv-files from the parent directory.

## Loading packages

This script is based mainly on the functions of the quanteda package. For the cross-validation of the textmodels, quanteda.classifiers has to be loaded from GitHub.

```{r packages, message=FALSE, warning=FALSE, results='hide'}

packages <- c(
  "quanteda", "quanteda.textmodels", "dplyr", "caret", "randomForest", "tm", "beepr", "rmarkdown", "e1071", "penalized", "plyr", "readr", "repr", "ggplot2", "rsample", "remotes", "stringr", "formatR", "readstata13", "lubridate", "reticulate")

# Using readstata13 because haven causes crash

lapply(packages[!(packages %in% rownames(installed.packages()))], install.packages)

if(!("quanteda.classifiers" %in% rownames(installed.packages()))) {
  remotes::install_github("quanteda/quanteda.classifiers")
} 

invisible(lapply(c(packages, "quanteda.classifiers"), require, character.only = T))

```

## Loading data

The sample data for Germany consists of 2,742 labelled press releases. The dataset is not  on GitHub and is loaded from the parent directory here.

```{r data}

sample_germany <- read.dta13("../sample_germany.dta", convert.factors = F)

# Correcting classification for three documents
sample_germany$issue[sample_germany$id == 229] <- 191
sample_germany$issue[sample_germany$id == 731] <- 7
sample_germany$issue[sample_germany$id == 902] <- 10

# Subset to relevant vars
germany_textpress <- sample_germany %>% select("header", "text", "issue", "position", "id")

# Distribution of issues in the hand-coded sample
table(germany_textpress$issue)

```



## Merging categories

In order to improve the classification, similar topics are merged or subsumed under the "Other" category. In practice, press releases regarding, for instance, Environment and Energy are often not distinguishable. Furthermore, small categories with very few observations are not suitable for automated classification.

```{r categories}
germany_textpress$issue_r1 <- as.numeric(germany_textpress$issue)

germany_textpress <- germany_textpress %>% mutate(issue_r1 = recode(issue_r1,
                           `8`  = 7,  # Environment & Energy
                           `13` = 10, # Transportation & Welfare
                           `14` = 10, # Housing & Welfare
                           `18` = 15, # Foreign Trade and Domestic Commerce
                           `98` = 99, # Non-thematic & Other
                           `23` = 99) # Culture: Too few observations
                                                  )
# Category descriptions
issue_categories <- data.frame(issue_r1 = c(1:7, 9:10, 12, 15:17, 20, 99, 191:192), issue_r1_descr = c("Macroeconomics", "Civil Rights", "Health", "Agriculture", "Labor", "Education", "Env. & Energy", "Immigration", "Welfare", "Law & Crime", "Commerce", "Defense", "Technology", "Gov. Ops.", "Other", "Int. Aff.", "EU"))

# Distribution with merged categories
table(germany_textpress$issue_r1)

```

## Creating the document frequency matrix (dfm)

We create a text corpus based on the header and text of each press release. We draw a random sample from the corpus to create a training and a test dataset. The test dataset consists of approx. one fifth of the documents.

Subsequently, we follow standard procedures for the preparation of the document frequency matrix. First, we remove stopwords and stem the words in order to better capture the similarities across documents. Second, we remove all punctuation, numbers, symbols and URLs. In a last step, we remove all words occurring in less than 0.5% or more than 90% of documents.

```{r dfm}
corp_press <- str_c(germany_textpress$header, " ", germany_textpress$text) %>% corpus()

# Add id var to corpus
docvars(corp_press, "id") <- germany_textpress$id
docvars(corp_press, "issue_r1") <- germany_textpress$issue_r1

# Create random sample for test dataset (size: 1/5 of all classified documents)
set.seed(300)
id_test <- sample(docvars(corp_press, "id"), 
                  round(length(docvars(corp_press, "id"))/5, 0), replace = FALSE)

# Create dfm
dfmat <- corpus_subset(corp_press) %>%
  dfm(remove = stopwords("de"), # Stem and remove stopwords, punctuation etc.
      stem = T, 
      remove_punct = T, 
      remove_number = T, 
      remove_symbols = T, 
      remove_url = T) %>% 
  dfm_trim(min_docfreq = 0.005, # Remove words occurring <.5% or > 80% of docs
           max_docfreq = .9, 
           docfreq_type = "prop") %>%
  suppressWarnings()

# Create training and test set 
dfmat_training <- dfm_subset(dfmat, !(id %in% id_test))
dfmat_test <- dfm_subset(dfmat, id %in% id_test)

# Make order of documents random
dfmat <- dfmat[sample(1:ndoc(dfmat), ndoc(dfmat)), ]

```

# Textmodels

## Multinomial Naive Bayes classification model

We calculate a Multinomial Naive Bayes text classification model. Multinomial NB models take into account the number of times a word occurs in a document, whereas Bernoulli NB models use the presence or absence of words only.

```{r nb}

# Five-fold cross-validation for every possible parameter combination
nb_eval <- textmodel_evaluate(dfmat, dfmat$issue_r1, k = 5, model = "textmodel_nb", fun = c("accuracy", "precision", "recall", "f1_score"), parameters = list(prior = c("uniform", "docfreq", "termfreq"), distribution = c("multinomial", "Bernoulli"), smooth = c(1, 2, 3)))
head(nb_eval)

aggregate(cbind(accuracy, precision, recall, f1_score, time, seed) ~ prior + distribution + smooth, nb_eval[, -c(1)], mean) %>% arrange(desc(accuracy))

```

Assuming a multinomial distribution of text features leads to a higher accuracy of the models compared to a Bernoulli distribution. The other benchmark parameters confirm this finding.

There is no clear pattern in regard to the effect of the priors on the quality of the models. A smoothing parameter of 1 for the feature counts seems optimal.


## SVM

Linear predictive models estimation based on the LIBLINEAR C/C++ Library.

(Not running here to save time.)

```{r svm}

# Five-fold cross-validation for every possible parameter combination
# svm_eval <- textmodel_evaluate(dfmat, dfmat$issue_r1, k = 5, model = "textmodel_svm", fun = c("accuracy", "precision", "recall", "f1_score"), parameters = list(weight = c("uniform", "docfreq", "termfreq"), type = c(0:7)))
# head(svm_eval)
# 
# aggregate(cbind(accuracy, precision, recall, f1_score, time, seed) ~ weight + type, svm_eval[, -c(1)], mean) %>% arrange(desc(accuracy))


```

None of the configurations lead to a higher accuracy compared to the benchmark NB model.

Regarding the type of linear models, type 0 (L2-regularized logistic regression, primal) and 7 (L2-regularized logistic regression, dual) seem to provide optimal results. There is no clear pattern regarding the weights.

The calculation of the models requires much more time/computing compared to the NB models.


## Next algorithm...

```{r next-algorithm}

# svm_eval <- textmodel_evaluate(dfmat, dfmat$issue_r1, k = 5, model = "textmodel_svm", fun = c("accuracy", "precision", "recall", "f1_score"), parameters = list(weight = c("uniform", "docfreq", "termfreq")))
# head(svm_eval)
# 
# aggregate(cbind(accuracy, time, seed) ~ weight, svm_eval[, -c(1)], mean) %>% arrange(desc(accuracy))


```

# Ensemble methods: SuperLearner

Prepare the datasets for the python code chunk.

```{r ensemble}

# Get vector indicating training sample
training <- !(1:ndoc(corp_press) %in% id_test)

# Subset training sample and write
if(!file.exists("../train_data.csv")) as.data.frame(as.matrix(dfm_trim(dfmat, min_docfreq = 15,
                        max_docfreq=0.80*nrow(dfmat), verbose = T))) %>%
  write.csv(train_data, "../train_data.csv")

# Get labels for training sample and write
if(!file.exists("../train_labels.csv")) write.csv(dfmat$issue_r1, "../train_labels.csv")


```

Run the SuperLearner in Python.

```{python superlearner}
# Load packages
import pandas as pd
import numpy as np

# Load sklearn tools
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load classifiers
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import BernoulliNB

# Load SuperLearner
from mlens.ensemble import SuperLearner

# create a list of base-models
def get_models():
	models = list()
	models.append(MultinomialNB())
	models.append(BernoulliNB())
	models.append(LogisticRegression(solver = 'liblinear', max_iter = 200))
	models.append(DecisionTreeClassifier())
	models.append(SVC(gamma = 'scale', probability = True))
	models.append(GaussianNB())
	models.append(KNeighborsClassifier())
	models.append(AdaBoostClassifier())
	models.append(BaggingClassifier())
	models.append(RandomForestClassifier())
	models.append(ExtraTreesClassifier())
	return models

# create the super learner
def get_super_learner(X):
	ensemble = SuperLearner(scorer = accuracy_score, folds = 5, shuffle = True, sample_size = len(X))
	
	# add base models
	models = get_models()
	ensemble.add(models)
	
	# add the meta model
	ensemble.add_meta(LogisticRegression(solver = 'liblinear'))
	return ensemble
 
# load press release data
train_data = pd.read_csv("../train_data.csv", index_col = 0)
train_labels = pd.read_csv("../train_labels.csv", index_col = 0)

X = train_data.values # Documents are already in random order
y = np.asarray([int(i) for i in train_labels.values])

# create the inputs and outputs

# split data
train, test, train_val, test_val = train_test_split(X, y, test_size = 0.20) # test_size = 0.50
print('Train', train.shape, train_val, 'Test', test.shape, test_val.shape)

# create the super learner
ensemble = get_super_learner(train)

# fit the super learner
ensemble.fit(train, train_val)

# summarize base learners
print(ensemble.data)

# make predictions on hold out set
test_hat = ensemble.predict(test)

print('Super Learner: %.3f' % (accuracy_score(test_val, test_hat) * 100))
```

The SuperLearner has a much lower accuracy than the base learners?!


# Classification of unlabelled data

## Using the NB textmodel

We trained the models using a set of 2,742 labelled documents. In order to obtain aggregated measures of issue attention, we predict the issue categories of all 47,111 labelled and unlabelled press releases in our sample.

```{r unlabelled}

tmod_nb_r1 <- textmodel_nb(dfmat_training, dfmat_training$issue_r1, distribution = "multinomial")

dfmat_matched <- dfm_match(dfmat_test, 
                           features = featnames(dfmat_training))

actual_class <- docvars(dfmat_matched, "issue_r1")
predicted_class <- predict(tmod_nb_r1, newdata = dfmat_matched)
tab_class <- table(actual_class, predicted_class)
tab_class

# Loading full dataset from parent dir
all_germany <- read_rds("../data_joint.RDS") %>% select(c(header, text.x, date.x, issue, party.x, id))

# Constructing the document frequency matrix
dfmat_all <- corpus(str_c(all_germany$header, " ", all_germany$text.x)) %>% 
  dfm(remove = stopwords("de"), # Stem and remove stopwords, punctuation etc.
      stem = T, 
      remove_punct = T, 
      remove_number = T, 
      remove_symbols = T, 
      remove_url = T)

# Adding docvars
docvars(dfmat_all, "party") <- all_germany$party.x
docvars(dfmat_all, "date") <- all_germany$date.x
docvars(dfmat_all, "id") <- all_germany$id

# Subsetting to features in the training data
dfmat_all <- dfm_match(dfmat_all, features = featnames(dfmat_training)) 

# Predicting the issue category for all documents
dfmat_all$issue_r1 <- predict(tmod_nb_r1, newdata = dfmat_all)

table(dfmat_all$issue_r1)

```


## Aggregation of the issues categories over time and party

To measure parties' evolving issue agendas, we aggregate the category counts over time.

```{r aggregation}

# Create dataframe from dfm
issue_agendas <- data.frame(date = docvars(dfmat_all, "date"), party = docvars(dfmat_all, "party"), issue_r1 = docvars(dfmat_all, "issue_r1"))

# Make date quarterly
issue_agendas$date <- as.character(issue_agendas$date) %>% substr(1, 8) %>% str_c("15") %>% str_replace_all(c("-01-" = "-02-", "-03-" = "-02-", "-04-" = "-05-", "-06-" = "-05-", "-07-" = "-08-", "-09-" = "-08-", "-10-" = "-11-", "-12-" = "-11-")) %>%  ymd()

# Add variable for counting
issue_agendas$freq <- 1

# Aggregate by party, date and issue
issue_agendas <- aggregate(freq ~ party + date + issue_r1, issue_agendas, sum)

# Add var for total press releases per party and month
issue_agendas$party_sum <- ave(issue_agendas$freq, issue_agendas$date, issue_agendas$party, FUN = sum)

issue_agendas$attention <- issue_agendas$freq / issue_agendas$party_sum

# Add issue descriptions
issue_agendas <- merge(issue_agendas, issue_categories, by = "issue_r1")
```


## Plotting issue attention: Examples

```{r plots, out.width = "80%"}

if(!dir.exists("plots")) dir.create("plots")

# Function for plotting parties' issue attention over time
plot_issue_party <- function(plot_issue, plot_party) ggplot(issue_agendas %>% filter(issue_r1 == plot_issue & party == plot_party), aes(x = date, y = attention)) +
  geom_step() +
  geom_smooth(method = "loess", formula = "y ~ x", color = "dark grey", lty = 3, se = F) +
  theme(axis.text.x = element_text(angle = 45, hjust=1)) +
  scale_x_date(date_minor_breaks = "1 year") +
  # ggtitle("Share of press releases for issue per quarter" %>% str_c(" (", issue_categories$issue_r1_descr[issue_categories$issue_r1 == plot_issue], " - ", plot_party, ")")) +
  ggsave(str_c("plots/", plot_issue, " - ", issue_categories$issue_r1_descr[issue_categories$issue_r1 == plot_issue], "_", plot_party, ".pdf"), device = cairo_pdf, width = 5*2^.5, height = 5) +
  ggsave(str_c("plots/", plot_issue, " - ", issue_categories$issue_r1_descr[issue_categories$issue_r1 == plot_issue],"_", plot_party, ".png"), width = 5*2^.5, height = 5)


# Plot quarterly issue attention for category "7 Environment & Energy" for "union_fraktion"
plot_issue_party(7,  "union_fraktion") # There seems to be a decline since Fukushima in 2011.

# Plot quarterly issue attention for category "9 Immigration" for "union_fraktion"
plot_issue_party(9,  "union_fraktion") # There seems to be a peak around the so-called refugee crisis in 2015.

# Plot quarterly issue attention for category "7 Environment & Energy" for "spd_fraktion"
plot_issue_party(7,  "spd_fraktion") # There seems to be a decline since Fukushima in 2011.

# Plot quarterly issue attention for category "10 Welfare" for "spd_fraktion"
plot_issue_party(10,  "spd_fraktion") # There seems to be a lower emphasis on welfare after the entry into government in fall 2013.


```