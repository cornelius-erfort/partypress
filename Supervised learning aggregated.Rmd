---
title: "Supervised learning aggregated"
author: "Cornelius Erfort"
date: "5/10/2021"
output: 
  pdf_document:
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, tidy.opts=list(width.cutoff = 80), tidy = T, python.reticulate = F)


```

# Setting up

This script requires the files "sample_germany.dta" and "data_joint.RDS" which are not included on GitHub. The superlearner model and the training/test csv-files are also ignored for GitHub.

## Loading packages

This script is based mainly on the functions of the quanteda package. For the cross-validation of the textmodels, quanteda.classifiers has to be loaded from GitHub.

```{r packages, message=FALSE, warning=FALSE, results='hide'}
start_time <- Sys.time()

packages <- c(
  "quanteda", "quanteda.textmodels", "dplyr", "caret", "randomForest", "tm", "rmarkdown", "plyr", "readr", "ggplot2", "stringr", "formatR", "readstata13", "lubridate", "reticulate", "doMC", "glmnet", "randomForest", "adabag", "kableExtra")

lapply(packages[!(packages %in% rownames(installed.packages()))], install.packages)

if(!("quanteda.classifiers" %in% rownames(installed.packages()))) {
  remotes::install_github("quanteda/quanteda.classifiers")
} 

invisible(lapply(c(packages, "quanteda.classifiers"), require, character.only = T))

```

## Loading data

The sample data for Germany consists of 2,740 labelled press releases. The dataset is not uploaded on GitHub.

```{r data, out.width = "80%"}

sample_germany <- read.dta13("data/sample_germany.dta", convert.factors = F)

# Correcting classification for three documents
sample_germany$issue[sample_germany$id == 229] <- 191
sample_germany$issue[sample_germany$id == 731] <- 7
sample_germany$issue[sample_germany$id == 902] <- 10

sample_germany <- filter(sample_germany, date != "NA" | !is.na(text))
nrow(sample_germany)

# Subset to relevant vars
germany_textpress <- sample_germany %>% select("header", "text", "issue", "position", "id", "party", "date")

# Distribution of issues in the hand-coded sample
table(germany_textpress$issue) %>% as.data.frame() %>% dplyr::rename(issue = Var1, n = Freq) %>% t() %>% kbl(booktabs = T) %>% 
  kable_styling(latex_options="scale_down")

```

## Merging categories

In order to improve the classification, similar topics are merged or subsumed under the "Other" category. In practice, press releases regarding, for instance, Environment and Energy are often not distinguishable. Furthermore, small categories with very few observations are not suitable for automated classification.

```{r categories}
germany_textpress$issue_r1 <- as.numeric(germany_textpress$issue)

germany_textpress <- germany_textpress %>% mutate(issue_r1 = recode(issue_r1,
                           `8`  = 7,  # Environment & Energy
                           `13` = 10, # Transportation & Welfare
                           `14` = 10, # Housing & Welfare
                           `18` = 15, # Foreign Trade and Domestic Commerce
                           `98` = 99, # Non-thematic & Other
                           `23` = 99) # Culture: Too few observations
                                                  )
# Category descriptions
issue_categories <- 
  data.frame(issue_r1 = c(1:7, 9:10, 12, 15:17, 20, 99, 191:192), 
             issue_r1_descr = c("Macroeconomics", "Civil Rights", 
                                "Health", "Agriculture", "Labor", "Education", "Env. & Energy", 
                                "Immigration", "Welfare", "Law & Crime", "Commerce", "Defense", 
                                "Technology", "Gov. Ops.", "Other", "Int. Aff.", "EU"))

issue_categories %>% dplyr::rename("Issue number" = issue_r1, "Issue name" = issue_r1_descr) %>% 
  kbl(booktabs = T)

# Distribution with merged categories
table(germany_textpress$issue_r1) %>% as.data.frame() %>% 
  dplyr::rename(issue = Var1, n = Freq) %>% t() %>% kbl(booktabs = T) %>% 
  kable_styling(latex_options="scale_down")

# Party names
party_names <- data.frame(party = c(1:8), 
                          party_name = c("Bündnis 90/Die Grünen - Fraktion", 
                                         "AfD - Bundesverband", "AfD - Fraktion", 
                                         "FDP - Bundesverband", "FDP - Fraktion", "DIE LINKE - Fraktion", 
                                         "SPD - Fraktion", "CDU/CSU - Fraktion"))
germany_textpress <- merge(germany_textpress, party_names, by = "party")

# Distribution by parties
table(germany_textpress$party_name) %>% as.data.frame() %>% 
  dplyr::rename(party = Var1, n = Freq) %>% kbl(booktabs = T)

table(germany_textpress$party_name, substr(germany_textpress$date, 6, 10)) %>% 
  as.data.frame.matrix() %>% kbl(booktabs = T)

```

## Creating the document frequency matrix (dfm)

We create a text corpus based on the header and text of each press release. We draw a random sample from the corpus to create a training and a test dataset. The test dataset consists of approx. one fifth of the documents.

Subsequently, we follow standard procedures for the preparation of the document frequency matrix. First, we remove stopwords and stem the words in order to better capture the similarities across documents. Second, we remove all punctuation, numbers, symbols and URLs. In a last step, we remove all words occurring in less than 0.5% or more than 90% of documents.


```{r dfm}
if(!dir.exists("train-test-data")) dir.create("train-test-data") # Delete folder when training and test data should be chosen anew
if(file.exists("train-test-data/dfmat.RData") & file.exists("train-test-data/dfmat_training.RData") & file.exists("train-test-data/dfmat_test.RData")) {
  load("train-test-data/dfmat.RData")
  load("train-test-data/dfmat_training.RData")
  load("train-test-data/dfmat_test.RData")
  } else {
  corp_press <- str_c(germany_textpress$header, " ", germany_textpress$text) %>% corpus()

# Add id var to corpus
docvars(corp_press, "id") <- germany_textpress$id
docvars(corp_press, "issue_r1") <- germany_textpress$issue_r1
docvars(corp_press, "party_name") <- germany_textpress$party_name

# Create dfm
dfmat <- corpus_subset(corp_press) %>%
  dfm(remove = stopwords("de"), # Stem and remove stopwords, punctuation etc.
      stem = T, 
      remove_punct = T, 
      remove_number = T, 
      remove_symbols = T, 
      remove_url = T) %>% 
  dfm_trim(min_docfreq = 0.005, # Remove words occurring <.5% or > 80% of docs
           max_docfreq = .9, 
           docfreq_type = "prop") %>%
  suppressWarnings()

# Make order of documents random
dfmat <- dfmat[sample(1:ndoc(dfmat), ndoc(dfmat)), ]
save(dfmat, file = "train-test-data/dfmat.RData")

# Create random sample for test dataset (size: 1/5 of all classified documents)
set.seed(300)
id_test <- sample(docvars(dfmat, "id"), 
                  round(length(docvars(dfmat, "id"))/5, 0), replace = FALSE)

# Create training and test set 
dfmat_training <- dfm_subset(dfmat, !(id %in% id_test))
save(dfmat_training, file = "train-test-data/dfmat_training.RData")
dfmat_test <- dfm_subset(dfmat, id %in% id_test)
save(dfmat_test, file = "train-test-data/dfmat_test.RData")
}



```

# Textmodels

Following Barberá et al. (2021) we estimate the following models:

1. Naive Bayes (multinomial)
2. Ridge regression (L2)
3. Lasso regression (L1)
4. Elastic Net
5. SVM
6. Random Forest

(Barberá, P., Boydstun, A., Linn, S., McMahon, R., & Nagler, J. (2021). Automated Text Classification of News Articles: A Practical Guide. Political Analysis, 29(1), 19-42. doi:10.1017/pan.2020.8)

We are mainly using the R packages quanteda and LiblineaR for the textmodels. For the SuperLearner ensemble we are replicating the models in Python with the sklearn library and use the mlens library to build the ensemble.

For most classifications we use a ten-fold cross-validation.

An overview of the results for each classifier can be found in the next section.

## Naive Bayes (multinomial)

We calculate a Multinomial Naive Bayes (NB) text classification model. Multinomial NB models take into account the number of times a word occurs in a document, whereas Bernoulli NB models use the presence or absence of words only.

We also test whether a Bernoulli NB model potentially outperforms the multinomial one.

```{r tm-naivebayes}
folds_cv <- 10 # Ten-fold cross-validation

# The textmodels are all saved in a directory. When the parameters are changed, the folder "textmodels" should be deleted in order to re-run the models.

# Ten-fold cross-validation for every possible parameter combination
if(!dir.exists("textmodels")) dir.create("textmodels")
filename <- "textmodels/naivebayes_eval.RData"
if(file.exists(filename)) load(filename) else {
  naivebayes_eval <- textmodel_evaluate(dfmat, dfmat$issue_r1, k = folds_cv, 
            model = "textmodel_nb", fun = c("accuracy", "precision", "recall", "f1_score"),
            parameters = list(prior = c("uniform", "docfreq", "termfreq"), distribution = c("multinomial", "Bernoulli"), smooth = c(1, 2, 3)))
  save(naivebayes_eval, file = filename)
}

(naivebayes_eval_aggr <- aggregate(cbind(accuracy, precision, recall, f1_score, time, seed) ~ prior + distribution + smooth, naivebayes_eval[, -c(1)], mean) %>% arrange(desc(accuracy)))[1:5, ] %>% mutate_at(vars(accuracy, precision, recall, f1_score), function (x) round(x, 3)) %>% kbl(booktabs = T)

naivebayes_eval_aggr$model <- "Naive bayes"

# Create a dataframe for all textmodels and save first row
tm_eval <- data.frame()
tm_eval <- naivebayes_eval_aggr[1, ] %>% rbind.fill(tm_eval)

```

Assuming a multinomial distribution of text features leads to a higher accuracy of the models compared to a Bernoulli distribution. The other benchmark parameters confirm this finding.

There is no clear pattern in regard to the effect of the priors on the quality of the models. A smoothing parameter of 1 for the feature counts seems optimal.


## Ridge regression (L2)

Lasso and Ridge regression mainly differ in the penalty term. While Lasso uses an absolute value as a penalty term, Ridge uses the squared value.

We estimate the L2-regularized logistic regression for our classification task (primal and dual).

```{r tm-ridge}
# Ten-fold cross-validation for every possible parameter combination
filename <- "textmodels/ridge_eval.RData"
if(file.exists(filename)) load(filename) else {
  ridge_eval <- textmodel_evaluate(dfmat, dfmat$issue_r1, k = folds_cv, model = "textmodel_svm", fun = c("accuracy", "precision", "recall", "f1_score"), 
      parameters = list(weight = c("uniform", "docfreq", "termfreq"), type = c(0, 7)))
    save(ridge_eval, file = filename)
}

(ridge_eval_aggr <- aggregate(cbind(accuracy, precision, recall, f1_score, time, seed) ~ weight + type, ridge_eval[, -c(1)], mean) %>% arrange(desc(accuracy)))[1:5, ] %>% mutate_at(vars(accuracy, precision, recall, f1_score), function (x) round(x, 3)) %>% kbl(booktabs = T)

ridge_eval_aggr$model <- "Ridge (L2)"

# Add first row to overall dataframe
tm_eval <- ridge_eval_aggr[1, ] %>% rbind.fill(tm_eval)


```

The classifier does not provide a higher accuracy compared to the baseline NB model.

The calculation of the models requires much more time/computing compared to the NB models.


## Lasso regression (L1)

We estimate the L1-regularized logistic regression for our classification task.

```{r tm-lasso}
# Ten-fold cross-validation for every possible parameter combination
filename <- "textmodels/lasso_eval.RData"
if(file.exists(filename)) load(filename) else {
  lasso_eval <- textmodel_evaluate(dfmat, dfmat$issue_r1, k = folds_cv, model = "textmodel_svm", fun = c("accuracy", "precision", "recall", "f1_score"), parameters = list(weight = c("uniform", "docfreq", "termfreq"), type = 6))
  save(lasso_eval, file = filename)
}

(lasso_eval_aggr <- aggregate(cbind(accuracy, precision, recall, f1_score, time, seed) ~ weight, lasso_eval[, -c(1)], mean) %>% arrange(desc(accuracy)))[1:2, ] %>% mutate_at(vars(accuracy, precision, recall, f1_score), function (x) round(x, 3)) %>% kbl(booktabs = T)

lasso_eval_aggr$model <- "Lasso (L1)"

# Add first row to overall dataframe
tm_eval <- lasso_eval_aggr[1, ] %>% rbind.fill(tm_eval)


```
The classifier does not lead to a higher accuracy compared to the baseline NB model and to a slightly lower accuracy than the ridge classifier.

The calculation of the models requires much more time/computing compared to the NB models.


## Elastic Net

The elastic net method combines the L1 and L2 penalties of the lasso and ridge methods (above).

```{r tm-elasticnet}
# Register multicore backend 
registerDoMC(cores = 4) 

filename <- "textmodels/elasticnet_eval.RData"
if(file.exists(filename)) load(filename) else {
  elasticnet_eval <- cv.glmnet(x = dfmat,
                   y = dfmat$issue_r1,
                   family = "multinomial", 
                   alpha = 0.5,
                   nfolds = folds_cv,
                   type.measure = "class",
                   parallel = T,
                   standardize = T)
  save(elasticnet_eval, file = filename)
}

elasticnet_eval$lambda.min

# Misclassification error
print(elasticnet_eval$cvm %>% min) 

# Accuracy
print(1 - elasticnet_eval$cvm %>% min) 

seed <- 1621447882
set.seed(seed)

# Estimate model
filename <- "textmodels/elasticnet_mod.RData"
if(file.exists(filename)) load(filename) else {
  eval_start <- Sys.time()
  elasticnet_mod <- glmnet(x = dfmat_training,
                   y = dfmat_training$issue_r1,
                   family = "multinomial",
                   alpha = 0.5,
                   type.measure = "class",
                   standardize = T)
    elasticnet_mod$time <- as.numeric(Sys.time() - eval_start)
save(elasticnet_mod, file = filename)
}


# Get lambda with best accuracy
elasticnet_pred <- predict(elasticnet_mod, newx = as.matrix(dfmat_test), type = "class")

acc_list <- apply(elasticnet_pred, MARGIN = 2, FUN = function (x)  accuracy(x, dfmat_test$issue_r1))

elasticnet_pred <- predict(elasticnet_mod, newx = as.matrix(dfmat_test), type = "class", s = elasticnet_mod$lambda[which(unlist(acc_list) == max(acc_list %>% unlist))][1])

tm_eval <- data.frame(accuracy = accuracy(elasticnet_pred, dfmat_test$issue_r1), 
           precision = precision(elasticnet_pred, dfmat_test$issue_r1) %>% unlist() %>% mean(),
           recall = recall(elasticnet_pred, dfmat_test$issue_r1) %>% unlist() %>% mean(),
           f1_score = f1_score(elasticnet_pred, dfmat_test$issue_r1) %>% unlist() %>% mean(),
           time = elasticnet_mod$time,
           seed = seed,
           model = "Elastic net",
           alpha = .5,
           distribution = "multinomial") %>% 
  rbind.fill(tm_eval)

```

The classifier does not provide a higher accuracy compared to the baseline NB model.

The calculation of the models requires much more time/computing compared to the NB models.


## SVM

We estimate support vector classification (Crammer and Singer 2001) for our classification task.

(Crammer, K. & Singer, Y. (2001). On the Algorithmic Implementation of Multiclass Kernel-based Vector Machines. Journal of Machine Learning Research, 2. 265-292.)

```{r tm-svm}
# Ten-fold cross-validation for every possible parameter combination
filename <- "textmodels/svm_eval.RData"
if(file.exists(filename)) load(filename) else {
  svm_eval <- textmodel_evaluate(dfmat, dfmat$issue_r1, k = folds_cv, model = "textmodel_svm", fun = c("accuracy", "precision", "recall", "f1_score"), parameters = list(weight = c("uniform", "docfreq", "termfreq"), type = 4))
  save(svm_eval, file =  filename)
}

(svm_eval_aggr <- aggregate(cbind(accuracy, precision, recall, f1_score, time, seed) ~ weight, svm_eval[, -c(1)], mean) %>% arrange(desc(accuracy)))[1:3, ] %>% mutate_at(vars(accuracy, precision, recall, f1_score), function (x) round(x, 3)) %>% kbl(booktabs = T)

svm_eval_aggr$model <- "SVM"

# Add first row to overall dataframe
tm_eval <- svm_eval_aggr[1, ] %>% rbind.fill(tm_eval)


```

None of the configurations lead to a higher accuracy compared to the baseline NB model.

There is no clear pattern regarding the choice of weights.

The calculation of the models requires more time/computing compared to the NB models.

## Random Forest

We estimate a model using the random forest algorithm for classification. We forgo cross-validation because OOB avoids over-classification.

(Breiman, L. (2001), Random Forests, Machine Learning 45(1), 5-32.)

```{r tm-randomforest}
# Estimate model (100 trees)
filename <- "textmodels/randomforest_eval.RData"
if(file.exists(filename)) load(filename) else {
  eval_start <- Sys.time()
  randomforest_eval <-  randomForest(x = as.matrix(dfmat_training), 
                   y = as.factor(dfmat_training$issue_r1),
                   xtest = as.matrix(dfmat_test),
                   ytest = as.factor(dfmat_test$issue_r1),
                   importance = T,
                   mtry = 20,
                   ntree = 100,
                   keep.forest = T,
                   type = "class")
  randomforest_eval$time <- as.numeric(Sys.time() - eval_start)
  save(randomforest_eval, file = filename)
}

randomforest_pred <- predict(randomforest_eval, newdata = as.matrix(dfmat_test), type = "class")

# Accuracy
print(accuracy(randomforest_pred, dfmat_test$issue_r1)) 

tm_eval <- data.frame(accuracy = accuracy(randomforest_pred, dfmat_test$issue_r1), 
           precision = precision(randomforest_pred, dfmat_test$issue_r1) %>% unlist() %>% mean(),
           recall = recall(randomforest_pred, dfmat_test$issue_r1) %>% unlist() %>% mean(),
           f1_score = f1_score(randomforest_pred, dfmat_test$issue_r1) %>% unlist() %>% mean(),
           time = randomforest_eval$time,
           seed = seed,
           model = "Random forest",
           alpha = .5,
           distribution = "multinomial") %>% 
  rbind.fill(tm_eval)


```

Random forest yields a lower accuracy than the baseline model and the computing time is much longer.

# Ensemble methods: SuperLearner

Prepare the datasets for the python code chunk.

```{r ensemble}
# Write training and test samples to csv (for Python to read)
if(!file.exists("train-test-data/train.csv")) as.data.frame(as.matrix(dfmat_training, verbose = T)) %>%
  write.csv(., "train-test-data/train.csv")
if(!file.exists("train-test-data/test.csv")) as.data.frame(as.matrix(dfmat_test, verbose = T)) %>%
  write.csv(., "train-test-data/test.csv")

# Get labels for training sample and write
if(!file.exists("train-test-data/train_val.csv")) write.csv(dfmat_training$issue_r1, "train-test-data/train_val.csv")
if(!file.exists("train-test-data/train-test-data/test_val.csv")) write.csv(dfmat_test$issue_r1, "train-test-data/test_val.csv")

```

Run SuperLearner in Python. (Additionally running AdaBoost.)

```{python superlearner}
# Load libraries
import os
import pandas as pd
import numpy as np
import pickle as pk
from sklearn.metrics import accuracy_score # Load sklearn tools
from mlens.ensemble import SuperLearner # Load SuperLearner

# Load classifiers
from sklearn.naive_bayes import MultinomialNB # 1
from sklearn.naive_bayes import BernoulliNB
# 2, 3 and 4 (logistic regression L2/L1 penalty and elastic net)
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC # 5
from sklearn.ensemble import RandomForestClassifier # 6
from sklearn.ensemble import AdaBoostClassifier # 7

# Set params
super_folds = 10
super_tol = .005

# Create a list of base-models
def get_models():
	models = list()
	models.append(MultinomialNB())
	models.append(BernoulliNB())
	models.append(LogisticRegression(solver = 'liblinear', 
	max_iter = 1000, tol = super_tol, penalty = "l2"))
	models.append(LogisticRegression(solver = 'liblinear', 
	max_iter = 1000, tol = super_tol, penalty = "l1"))
	models.append(LogisticRegression(solver = 'saga', max_iter = 1000, 
	penalty = 'elasticnet', l1_ratio = .5, multi_class = 'multinomial', 
	random_state = np.random.seed(3027), tol = super_tol))
	models.append(SVC(probability = True, tol = super_tol))
	models.append(RandomForestClassifier())
	models.append(AdaBoostClassifier())
	return models

# Create the superlearner
def get_super_learner(X):
	ensemble = SuperLearner(scorer = None, folds = super_folds, shuffle = True, 
	random_state = np.random.seed(3027), sample_size = len(train_val), 
	n_jobs = 1, verbose = True)
	models = get_models() # Add base models
	ensemble.add(models, proba = True)
	ensemble.add_meta(LogisticRegression(solver = 'lbfgs', 
	max_iter = 1000, tol = super_tol), proba = False) # Add the meta model
	return ensemble
 
# Load press release data (training and test)
train = pd.read_csv("train-test-data/train.csv", index_col = 0).values
train_val = np.asarray([int(i) for i in pd.read_csv("train-test-data/train_val.csv", 
index_col = 0).values])

test = pd.read_csv("train-test-data/test.csv", index_col = 0).values
test_val = np.asarray([int(i) for i in pd.read_csv("train-test-data/test_val.csv", 
index_col = 0).values])

# Create the super learner
ensemble = get_super_learner(train)

# Fit the super learner
filename = "textmodels/superlearner.PyData"
if os.path.exists(filename) != True:
  ensemble.fit(train, train_val)
  pk.dump(ensemble, open(filename, 'wb'))
else:
  ensemble = pk.load(open(filename, 'rb'))

# Summarize base learners
print(ensemble.data)

# Make predictions on test data
filename = "textmodels/super_pred.PyData"
if os.path.exists(filename) != True:
  super_pred = ensemble.predict(test)
  pk.dump(super_pred, open(filename, 'wb'))
  np.savetxt("textmodels/super_pred.csv", super_pred, delimiter = ",")
else:
  super_pred = pk.load(open(filename, 'rb'))

```

```{r superlearner-r}
# Load superlearner prediction and add row to evaluation table
super_pred <- read_csv("textmodels/super_pred.csv", col_names = F)[[1]] %>% as.numeric()

tm_eval <- data.frame(accuracy = accuracy(super_pred, dfmat_test$issue_r1),
           precision = precision(super_pred, dfmat_test$issue_r1) %>% unlist() %>% mean(),
           recall = recall(super_pred, dfmat_test$issue_r1) %>% unlist() %>% mean(),
           f1_score = f1_score(super_pred, dfmat_test$issue_r1) %>% unlist() %>% mean(),
           # time = as.numeric(Sys.time() - super_start),
           model = "SuperLearner ensemble") %>%
  rbind.fill(tm_eval)

```

The SuperLearner does not significantly increase the accuracy, but the necessary computing time is much greater.

# Evaluation of textmodels

In this section, we present a table for comparison of our textmodels.

```{r  tm-eval}

# Comparison of textmodels
tm_eval[, c("accuracy", "precision", "recall", "f1_score")] <- apply(tm_eval[, c("accuracy", "precision", "recall", "f1_score")], MARGIN = 2, function (x) round(x, 3))
tm_eval[order(tm_eval$accuracy, decreasing = T), c("model", "accuracy", "precision", "recall", "f1_score", "distribution")] %>% kbl(booktabs = T, row.names = F)

# Naive Bayes baseline model
baseline_nb <- textmodel_nb(dfmat_training, dfmat_training$issue_r1, distribution = "multinomial")

# Confusion matrix and overall statistics
table(dfmat_test$issue_r1, predict(baseline_nb, newdata = dfmat_test)) %>%
  confusionMatrix(mode = "everything")

```

The Naive Bayes model reaches a high accuracy of ~.64 which can only be slightly improved using an ensemble of classifiers (~.68). We thus rely on the NB classifier for the classification of press releases.

Regarding the issues, the classifier works better for specific issue categories.
While some have a higher than average sensitivity (e.g. 3 - Health, 5 - Labor, 6 - Education, 7 - Env. & Energy, 9 - Immigration, 191 - Int. Affairs), others fare worse than average (e.g. 15 - Commerce, 17 - Technology, 20 - Gov. Ops., 99 - Other). Unsurprisingly, the rather unspecifict categories 20 - Gov. Ops. and 99 - Other are difficult to predict. 

The better sensitivity for other categories is likely the result of a more specific use of words. Category 17 - Technology may feature a worse accuracy because it is underrepresented in the labelled data. Category 15 - Commerce is often misclassified as 1 - Macroeconomics, 4 - Agriculture, 17 - Technology and 191 - Int. Affairs: All categories where a press release may contain similar words.


# Classification of unlabelled data

## Using the NB textmodel

We trained the models using a set of 2,740 labelled documents. In order to obtain aggregated measures of issue attention, we predict the issue categories of all 47,111 labelled and unlabelled press releases in our sample.

```{r unlabelled}

# Naive Bayes with full labelled data
tm_naivebayes <- textmodel_nb(dfmat, dfmat$issue_r1, distribution = "multinomial")

# Loading full dataset from hidden dir
all_germany <- read_rds("data/data_joint.RDS") %>% select(c(header, text.x, date.x, issue, party.x, id)) %>% filter(!is.na(date.x) & !is.na(text.x))
nrow(all_germany)

# Constructing the document frequency matrix
dfmat_all <- corpus(str_c(all_germany$header, " ", all_germany$text.x)) %>% 
  dfm(remove = stopwords("de"), # Stem and remove stopwords, punctuation etc.
      stem = T, 
      remove_punct = T, 
      remove_number = T, 
      remove_symbols = T, 
      remove_url = T) %>% suppressWarnings()

# Adding docvars
docvars(dfmat_all, "party") <- all_germany$party.x
docvars(dfmat_all, "date") <- all_germany$date.x
docvars(dfmat_all, "id") <- all_germany$id

# Subsetting to features in the training data
dfmat_all <- dfm_match(dfmat_all, features = featnames(dfmat_training)) 

# Predicting the issue category for all documents
dfmat_all$issue_r1 <- predict(tm_naivebayes, newdata = dfmat_all)

table(dfmat_all$issue_r1) %>% as.data.frame() %>% 
  dplyr::rename(issue = Var1, n = Freq) %>% t() %>% kbl(booktabs = T) %>% 
  kable_styling(latex_options="scale_down")

```


## Aggregation of the issues categories over time and party

To measure parties' evolving issue agendas, we aggregate the category counts over time.

```{r aggregation}

# Create dataframe from dfm
issue_agendas <- data.frame(date = docvars(dfmat_all, "date"), party = docvars(dfmat_all, "party"), issue_r1 = docvars(dfmat_all, "issue_r1"))

# Make date quarterly
issue_agendas$date <- as.character(issue_agendas$date) %>% substr(1, 8) %>% str_c("15") %>% str_replace_all(c("-01-" = "-02-", "-03-" = "-02-", "-04-" = "-05-", "-06-" = "-05-", "-07-" = "-08-", "-09-" = "-08-", "-10-" = "-11-", "-12-" = "-11-")) %>%  ymd()

# Add variable for counting
issue_agendas$freq <- 1

# Aggregate by party, date and issue
issue_agendas <- aggregate(freq ~ party + date + issue_r1, issue_agendas, sum)

# Add var for total press releases per party and month
issue_agendas$party_sum <- ave(issue_agendas$freq, issue_agendas$date, issue_agendas$party, FUN = sum)

issue_agendas$attention <- issue_agendas$freq / issue_agendas$party_sum

# Add issue descriptions
issue_agendas <- merge(issue_agendas, issue_categories, by = "issue_r1")
```


## Plotting issue attention: Examples

```{r plots, out.width = "80%"}

if(!dir.exists("plots")) dir.create("plots")

# Function for plotting parties' issue attention over time
plot_issue_party <- function(plot_issue, plot_party) ggplot(issue_agendas %>% filter(issue_r1 == plot_issue & party == plot_party), aes(x = date, y = attention)) +
  geom_step() +
  geom_smooth(method = "loess", formula = "y ~ x", color = "dark grey", lty = 3, se = F) +
  theme(axis.text.x = element_text(angle = 45, hjust=1)) +
  scale_x_date(date_minor_breaks = "1 year") +
  ggsave(str_c("plots/", plot_issue, " - ", 
               issue_categories$issue_r1_descr[issue_categories$issue_r1 == plot_issue], "_", plot_party, ".pdf"), 
         device = cairo_pdf, width = 5*2^.5, height = 5) +
  ggsave(str_c("plots/", plot_issue, " - ", 
               issue_categories$issue_r1_descr[issue_categories$issue_r1 == plot_issue],"_", plot_party, ".png"), 
         width = 5*2^.5, height = 5)


# Plot quarterly issue attention for category "7 Environment & Energy" for "union_fraktion"
plot_issue_party(7,  "union_fraktion") 
# There seems to be a decline since Fukushima in 2011.

# Plot quarterly issue attention for category "9 Immigration" for "union_fraktion"
plot_issue_party(9,  "union_fraktion") 
# There seems to be a peak around the so-called refugee crisis in 2015.

# Plot quarterly issue attention for category "7 Environment & Energy" for "spd_fraktion"
plot_issue_party(7,  "spd_fraktion") 
# There seems to be a decline since Fukushima in 2011.

# Plot quarterly issue attention for category "10 Welfare" for "spd_fraktion"
plot_issue_party(10,  "spd_fraktion") 
# There seems to be a lower emphasis on welfare after the entry into government in fall 2013.


# Time needed to run script (much shorter when textmodels are just loaded from a file)
print(Sys.time() - start_time) 
```