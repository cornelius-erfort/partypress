---
title: "Preparing the textual data"
author: "Cornelius Erfort"
date: "9 Aug 2021"
output: 
  pdf_document:
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, tidy.opts=list(width.cutoff = 80), tidy = T, python.reticulate = F)
knitr::opts_knit$set(root.dir = dirname(getwd()))

```

# Setting up

This script requires the files which are not included on GitHub.

## Loading packages

This script is based mainly on the functions of the quanteda package. For the cross-validation of the textmodels, quanteda.classifiers has to be loaded from GitHub.

```{r packages, message=FALSE, warning=FALSE, results='hide'}
start_time <- Sys.time()

packages <- c(
  "quanteda", "dplyr", "tm", "rmarkdown", "plyr", "readr", "ggplot2", "stringr", "formatR", "readstata13", "lubridate", "kableExtra", "stargazer", "xlsx")

lapply(packages[!(packages %in% rownames(installed.packages()))], install.packages)
invisible(lapply(packages, require, character.only = T))

theme_update(text = element_text(family = "LM Roman 10")) # Set font family for ggplot

set.seed(4325)
```

## Loading data

For the evaluation of classifiers, we use the data for Germany.

The data for Germany consists of 2,612 labeled press releases. The dataset is not uploaded on GitHub.

```{r data, out.width = "80%"}
labeled <- read.xlsx("data/labeled/germany-labeled.xlsx", sheetIndex = 1) %>%
  suppressWarnings()
nrow(labeled)

# Clean
labeled <- labeled %>% mutate(issue = issue %>% as.numeric())

table(labeled$issue, useNA = "always")

# Subset to relevant vars
textpress <- labeled %>% select("header", "text", "issue", "position", "id", "party", "date")
rm(labeled)

# Distribution of issues in the hand-coded sample
table(textpress$issue) %>% as.data.frame() %>% dplyr::rename(issue = Var1, n = Freq) %>% t() %>% kbl(booktabs = T) %>% 
  kable_styling(latex_options = "scale_down")

```

## Prepare data

Add issue category names, unify parties, add variable for cross-validation.

```{r prepare}

# Category descriptions
issue_categories <- 
  data.frame(issue = c(1:10, 12:18, 20, 23, 98, 99, 191:192), 
             issue_descr = c("Macroeconomics", "Civil Rights", 
                                "Health", "Agriculture", "Labor", "Education", "Environment", "Energy", 
                                "Immigration", "Transportation", "Law and Crime", "Social Welfare",
                             "Housing", "Domestic Commerce", "Defense", "Technology", 
                             "Foreign Trade", "Government Operations", "Culture", "Non-thematic", "Other", "International Affairs", "European Integration"))
issue_categories %>% dplyr::rename("Issue number" = issue, "Issue name" = issue_descr) %>% 
  kbl(booktabs = T)


# Party names
party_names <- data.frame(party = c("90gruene_fraktion", 
                                    "afd_bundesverband", "afd_fraktion",
                                    "fdp_bundesverband", "fdp_fraktion",
                                    "linke_fraktion", "spd_fraktion",
                                    "union_fraktion"), 
                          party_name = c("Bündnis 90/Die Grünen - Fraktion", 
                                         "AfD - Bundesverband", "AfD - Fraktion", 
                                         "FDP - Bundesverband", "FDP - Fraktion", 
                                         "DIE LINKE - Fraktion", "SPD - Fraktion", 
                                         "CDU/CSU - Fraktion"))
textpress <- merge(textpress, party_names, by = "party")

# Distribution by parties
table(textpress$party_name) %>% as.data.frame() %>% 
  dplyr::rename(party = Var1, n = Freq) %>% kbl(booktabs = T)

table(textpress$party_name, substr(textpress$date, 1, 4)) %>% 
  as.data.frame.matrix() %>% kbl(booktabs = T)

# Combine header and text
textpress$htext <- str_c(textpress$header, " ", textpress$text)

# Make order of documents random
textpress <- textpress[sample(1:nrow(textpress), nrow(textpress)), ]
textpress$cv_sample <- sample(1:5, nrow(textpress), replace = T)

if(!dir.exists("supervised-files")) dir.create("supervised-files")
if(!dir.exists("supervised-files/data")) dir.create("supervised-files/data")

# Save dataframe (do not overwrite because the cross-validation folds are saved here)
if(!file.exists("supervised-files/data/textpress.RData")) save(textpress, file = "supervised-files/data/textpress.RData") else{
  load("supervised-files/data/textpress.RData")
}
```


# Supervised models
## Creating the document frequency matrix (dfm)

We create a text corpus based on the header and text of each press release. We draw a random sample from the corpus to create a training and a test dataset. The test dataset consists of approx. one fifth of the documents.

Subsequently, we follow standard procedures for the preparation of the document frequency matrix. First, we remove stopwords and stem the words in order to better capture the similarities across documents. Second, we remove all punctuation, numbers, symbols and URLs. In a last step, we remove all words occurring in less than 0.5% or more than 90% of documents.


```{r dfm}
if(!dir.exists("supervised-files")) dir.create("supervised-files")

if(file.exists("supervised-files/data/dfmat.RData")) {
   load("supervised-files/data/dfmat.RData")
   load("supervised-files/data/dfmat_alt.RData")
} else {
    
corp_press <- corpus(str_c(textpress$header, " ", textpress$text),
                       docvars = select(textpress, c(id, issue, party_name, cv_sample)))

# Create dfm
dfmat <- corpus_subset(corp_press) %>%
  dfm(remove = stopwords("de"), # Stem and remove stopwords, punctuation etc.
      stem = T, remove_punct = T, remove_number = T, remove_symbols = T, remove_url = T) %>% 
  dfm_trim(min_docfreq = 0.005, max_docfreq = .9, # Remove words occurring <.5% or > 80% of docs
           docfreq_ = "prop") %>%
  suppressWarnings()
save(dfmat, file = "supervised-files/data/dfmat.RData")



# Create alternative dfm (bigrams and tfidf)
dfmat_alt <- corpus_subset(corp_press) %>%
  tokens() %>% tokens_ngrams(n = 1:2) %>%
  dfm(remove = stopwords("de"), # Stem and remove stopwords, punctuation etc.
      stem = T, remove_punct = T, remove_number = T, remove_symbols = T, remove_url = T) %>% 
  dfm_trim(max_docfreq = .06, # Remove words occurring >6% of docs
           docfreq_ = "prop") %>%
   dfm_trim(min_docfreq = 5, # Remove words occurring in <5 docs
           docfreq_ = "count") %>% suppressWarnings()

save(dfmat_alt, file = "supervised-files/data/dfmat_alt.RData")

}

```

# Superlearner
``` {r superlearner}
if(!dir.exists("superlearner-files")) dir.create("superlearner-files")

## Create training and test set (also as csv for Python)
cbind(cv_sample = dfmat$cv_sample, label = dfmat$issue, as.data.frame(dfmat)) %>% select(-c(doc_id)) %>% write.csv("superlearner-files/dfmat.csv", row.names = F)

## Create training and test set (also as csv for Python)
cbind(cv_sample = dfmat_alt$cv_sample, label = dfmat_alt$issue, as.data.frame(dfmat_alt)) %>% select(-c(doc_id)) %>% write.csv("superlearner-files/dfmat_alt.csv", row.names = F)

```

# Readme, semi-supervised, and transfer models

Generate csv file with unlabeled and labeled documents.

```{r semi-transfer}
if(!dir.exists("semi-files")) dir.create("semi-files")
if(!dir.exists("transfer-files")) dir.create("transfer-files")
if(!dir.exists("readme-files")) dir.create("readme-files")


# Load all press releases
load("data/all/germany.RData")
alldocs <- germany %>% select(country, party, date, header, text, id)
nrow(alldocs) # 44,950
names(alldocs)

# Add labels and folds by id from textpress
load("supervised-files/data/textpress.RData")
alldocs <- merge(alldocs, select(textpress, c(id, issue, cv_sample)), by = "id", all = T)
nrow(alldocs) # 44,950

alldocs$cv_sample[is.na(alldocs$cv_sample)] <- -1


# alldocs[!(alldocs$id %in% germany$id), ] %>% View

# Combine header and text
alldocs$htext <- str_c(alldocs$header, " ", alldocs$text)
alldocs <- select(alldocs, -c(header, text))

alldocs$issue[is.na(alldocs$issue)] <- -1

# Show distribution of text length (labeled data)
sapply(alldocs$htext[alldocs$issue != -1], str_length) %>% density() %>% plot

# Count words/tokens
sapply(alldocs$htext, function(x) lengths(gregexpr("\\W+", x)) + 1) %>% summary # max_seq_length = 512

# Make issues compatible with transformers (0-23 instead of CAP labels)
labels <- data.frame(issue = unique(alldocs$issue) %>% sort, label = c(-1, 0:22))
alldocs <- merge(alldocs, labels, by = "issue", all.x = T)


table(alldocs$issue, useNA = "ifany")


# Write to csv

if(!file.exists("transfer-files/alldocs.csv")) write_csv(alldocs, file = "transfer-files/alldocs.csv")

if(!file.exists("semi-files/alldocs.csv")) write_csv(select(alldocs, -c(label)), file = "semi-files/alldocs.csv")

alldocs <- select(alldocs, -c(label))

if(!file.exists("readme-files/alldocs.RData")) save(alldocs, file = "readme-files/alldocs.RData")


```


```{r script_eval}
# Time needed to run script
print(Sys.time() - start_time) 
```