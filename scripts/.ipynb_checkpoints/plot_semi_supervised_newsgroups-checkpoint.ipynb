{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Semi-supervised Classification on a Text Dataset\n",
    "\n",
    "In this example, semi-supervised classifiers are trained on the 20 newsgroups\n",
    "dataset (which will be automatically downloaded).\n",
    "\n",
    "You can adjust the number of categories by giving their names to the dataset\n",
    "loader or setting them to `None` to get all 20 of them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.semi_supervised import SelfTrainingClassifier\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314 documents\n",
      "20 categories\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = fetch_20newsgroups(subset='train', categories=None)\n",
    "print(\"%d documents\" % len(data.filenames))\n",
    "print(\"%d categories\" % len(data.target_names))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "sdg_params = dict(alpha=1e-5, penalty='l2', loss='log')\n",
    "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Supervised Pipeline\n",
    "pipeline = Pipeline([\n",
    "    #('vect', CountVectorizer(**vectorizer_params)),\n",
    "    #('tfidf', TfidfTransformer()),\n",
    "    #('clf', SGDClassifier(**sdg_params)),\n",
    "    ('clf', MultinomialNB()),\n",
    "\n",
    "])\n",
    "# SelfTraining Pipeline\n",
    "st_pipeline = Pipeline([\n",
    "    #('vect', CountVectorizer(**vectorizer_params)),\n",
    "    #('tfidf', TfidfTransformer()),\n",
    "    #('clf', SelfTrainingClassifier(SGDClassifier(**sdg_params), verbose=True)),\n",
    "    ('clf', SelfTrainingClassifier(MultinomialNB(), verbose=True)),\n",
    "])\n",
    "# LabelSpreading Pipeline\n",
    "ls_pipeline = Pipeline([\n",
    "    #('vect', CountVectorizer(**vectorizer_params)),\n",
    "    #('tfidf', TfidfTransformer()),\n",
    "    # LabelSpreading does not support dense matrices\n",
    "    ('todense', FunctionTransformer(lambda x: x.todense())),\n",
    "    ('clf', LabelSpreading()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def eval_and_print_metrics(clf, X_train, y_train, X_test, y_test):\n",
    "    print(\"Number of training samples:\", len(X_train))\n",
    "    print(\"Unlabeled samples in training set:\",\n",
    "          sum(1 for x in y_train if x == -1))\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"Accuracy score on test set: \"\n",
    "          \"%0.3f\" % accuracy_score(y_test, y_pred))\n",
    "    print(\"-\" * 10)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised SGDClassifier on 100% of the data:\n",
      "Number of training samples: 8485\n",
      "Unlabeled samples in training set: 0\n",
      "Micro-averaged F1 score on test set: 0.909\n",
      "----------\n",
      "\n",
      "Supervised SGDClassifier on 20% of the training data:\n",
      "Number of training samples: 1682\n",
      "Unlabeled samples in training set: 0\n",
      "Micro-averaged F1 score on test set: 0.792\n",
      "----------\n",
      "\n",
      "SelfTrainingClassifier on 20% of the training data (rest is unlabeled):\n",
      "Number of training samples: 8485\n",
      "Unlabeled samples in training set: 6803\n",
      "End of iteration 1, added 2844 new labels.\n",
      "End of iteration 2, added 692 new labels.\n",
      "End of iteration 3, added 216 new labels.\n",
      "End of iteration 4, added 73 new labels.\n",
      "End of iteration 5, added 38 new labels.\n",
      "End of iteration 6, added 16 new labels.\n",
      "End of iteration 7, added 8 new labels.\n",
      "End of iteration 8, added 5 new labels.\n",
      "End of iteration 9, added 6 new labels.\n",
      "End of iteration 10, added 8 new labels.\n",
      "Micro-averaged F1 score on test set: 0.836\n",
      "----------\n",
      "\n",
      "LabelSpreading on 20% of the data (rest is unlabeled):\n",
      "Number of training samples: 8485\n",
      "Unlabeled samples in training set: 6803\n",
      "Micro-averaged F1 score on test set: 0.631\n",
      "----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"Supervised SGDClassifier on 100% of the data:\")\n",
    "    eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # select a mask of 20% of the train dataset\n",
    "    y_mask = np.random.rand(len(y_train)) < 0.2\n",
    "\n",
    "    # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
    "    X_20, y_20 = map(list, zip(*((x, y)\n",
    "                     for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
    "    print(\"Supervised SGDClassifier on 20% of the training data:\")\n",
    "    eval_and_print_metrics(pipeline, X_20, y_20, X_test, y_test)\n",
    "\n",
    "    # set the non-masked subset to be unlabeled\n",
    "    y_train[~y_mask] = -1\n",
    "    print(\"SelfTrainingClassifier on 20% of the training data (rest \"\n",
    "          \"is unlabeled):\")\n",
    "    eval_and_print_metrics(st_pipeline, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    if 'CI' not in os.environ:\n",
    "        # LabelSpreading takes too long to run in the online documentation\n",
    "        print(\"LabelSpreading on 20% of the data (rest is unlabeled):\")\n",
    "        eval_and_print_metrics(ls_pipeline, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(y_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "os.chdir(\"/Users/cornelius/Desktop/SCRIPTS/GitHub/scripts-issue-agendas\")\n",
    "os.getcwd()\n",
    "\n",
    "# Load press release data (training and test)\n",
    "X_train = pd.read_csv(\"semi-files/X_train.csv\", index_col = 0).values\n",
    "y_train = np.asarray([int(i) for i in pd.read_csv(\"semi-files/y_train.csv\", \n",
    "index_col = 0).values])\n",
    "\n",
    "X_test = pd.read_csv(\"semi-files/X_test.csv\", index_col = 0).values\n",
    "y_test = np.asarray([int(i) for i in pd.read_csv(\"semi-files/y_test.csv\", \n",
    "index_col = 0).values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2232"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train[(y_train != -1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2232"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[(y_train != -1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBmod = MultinomialNB()\n",
    "NBmod.fit(X_train[(y_train != -1)][:1000], y_train[(y_train != -1)][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_new = y_train\n",
    "y_train_new[(y_train != -1)][1001:] = NBmod.predict(X_train[(y_train != -1)][1001:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NBmod2 = MultinomialNB()\n",
    "NBmod2.fit(X_train[(y_train_new != -1)], y_train[(y_train_new != -1)])\n",
    "\n",
    "X_train[(y_train != -1)][1001:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5374531835205992"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(NBmod.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.semi_supervised import SelfTrainingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import ComplementNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelfTrainingClassifier(base_estimator=SVC(gamma='auto', probability=True))"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC(probability=True, gamma=\"auto\")\n",
    "\n",
    "self_training_model = SelfTrainingClassifier(svc, verbose = True)\n",
    "\n",
    "self_training_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(self_training_model.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of iteration 1, added 817 new labels.\n",
      "End of iteration 2, added 116 new labels.\n",
      "End of iteration 3, added 14 new labels.\n",
      "End of iteration 4, added 2 new labels.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SelfTrainingClassifier(base_estimator=ComplementNB(), verbose=True)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = ComplementNB()\n",
    "\n",
    "self_training_model_gnb = SelfTrainingClassifier(svc, verbose = True)\n",
    "\n",
    "self_training_model_gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6235955056179775"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(self_training_model_gnb.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cornelius/opt/anaconda3/lib/python3.8/site-packages/sklearn/semi_supervised/_self_training.py:187: UserWarning: y contains no unlabeled samples\n",
      "  warnings.warn(\"y contains no unlabeled samples\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SelfTrainingClassifier(base_estimator=MultinomialNB(), verbose=True)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = MultinomialNB()\n",
    "\n",
    "self_training_model_mnb = SelfTrainingClassifier(svc, verbose = True)\n",
    "\n",
    "self_training_model_mnb.fit(X_train[y_train != -1][:1000], y_train[y_train != -1][:1000])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of iteration 1, added 1369 new labels.\n",
      "End of iteration 2, added 85 new labels.\n",
      "End of iteration 3, added 17 new labels.\n",
      "End of iteration 4, added 3 new labels.\n",
      "End of iteration 5, added 4 new labels.\n",
      "End of iteration 6, added 2 new labels.\n",
      "End of iteration 7, added 1 new labels.\n",
      "End of iteration 8, added 3 new labels.\n",
      "End of iteration 9, added 3 new labels.\n",
      "End of iteration 10, added 2 new labels.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SelfTrainingClassifier(base_estimator=SGDClassifier(alpha=1e-05, loss='log'),\n",
       "                       verbose=True)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SGDClassifier(alpha=1e-5, penalty='l2', loss='log')\n",
    "\n",
    "self_training_model_sdg = SelfTrainingClassifier(svc, verbose = True)\n",
    "\n",
    "self_training_model_sdg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5299625468164794"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(self_training_model_sdg.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2232"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train[y_train!= -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # X, y = data.data, data.target\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    print(\"Supervised SGDClassifier on 100% of the data:\")\n",
    "    eval_and_print_metrics(pipeline, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # select a mask of 20% of the train dataset\n",
    "    y_mask = np.random.rand(len(y_train)) < 0.2\n",
    "\n",
    "    # X_20 and y_20 are the subset of the train dataset indicated by the mask\n",
    "    X_20, y_20 = map(list, zip(*((x, y)\n",
    "                     for x, y, m in zip(X_train, y_train, y_mask) if m)))\n",
    "    print(\"Supervised SGDClassifier on 20% of the training data:\")\n",
    "    eval_and_print_metrics(pipeline, X_20, y_20, X_test, y_test)\n",
    "\n",
    "    # set the non-masked subset to be unlabeled\n",
    "    y_train[~y_mask] = -1\n",
    "    print(\"SelfTrainingClassifier on 20% of the training data (rest \"\n",
    "          \"is unlabeled):\")\n",
    "    eval_and_print_metrics(st_pipeline, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    if 'CI' not in os.environ:\n",
    "        # LabelSpreading takes too long to run in the online documentation\n",
    "        print(\"LabelSpreading on 20% of the data (rest is unlabeled):\")\n",
    "        eval_and_print_metrics(ls_pipeline, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: jll = -44409836.144086\n",
      "Step 2: jll = -44592354.743390\n",
      "Step 3: jll = -44601743.852176\n",
      "Step 4: jll = -44606733.320179\n",
      "Step 5: jll = -44611595.432071\n",
      "Step 6: jll = -44611439.174188\n",
      "Optimization converged after 6 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNBSS()"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NBmodSS = MultinomialNBSS()\n",
    "NBmodSS.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(NBmodSS.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10,  12,  16,   5,   5,  15,  10,  10,   7,  99,   2,   2,   9,\n",
       "        15,  15,   6,   7,   7,   1,  10,  99,  15,   5,   7, 191, 191,\n",
       "         7,  12,   7,  12,   6,   4,  12,   7,   1,  15,   6,  12,   7,\n",
       "        15,   7,   5,   5,  12,   7,  99,  10, 191,   6,   6,   7,  12,\n",
       "        12,  12,   7,   5,   5,   4,  15,  12,  10,  15,   1,   4,   6,\n",
       "        99,  15,   6,  10,   7,   7,  15,   7,   1, 191, 191,   6,   7,\n",
       "       192,   5,  15, 191,   7,  10,   5,  12,   5, 191,   1,   2,   6,\n",
       "         5,   4,   5,  12, 191, 191,  15,   4, 191,  15,   7, 191,   5,\n",
       "       191,   7,   2,  10,   7, 191,   4,  12,  12,   5,   4,  15,  12,\n",
       "        99,   5,  99,   4, 191,   7,  15,   5, 191,   7,  10,  12,  12,\n",
       "        15, 191,   6,   5,  12,  15,   7,   7,   1,  16, 191,  15,   1,\n",
       "         2, 191, 191,  12, 191,   7,   7,  10,   7,   7,   5,   7, 191,\n",
       "         7,  15,   7,  10,   7,  12,  15,  16,   5,  12,   5,  12,   7,\n",
       "         7,  12,   6,  10,  10,  12,   5,   4,  10,   4,  12,  12,   6,\n",
       "       191,   4,   5, 191,  10,  15,  15,   7,  99,  12,  12,   5,   4,\n",
       "        10, 191,   5,  12,   7, 191,  12,   4,  15, 191,  12, 191,  99,\n",
       "         5,   7, 191,   7,  15,  12,  15,  15,  15,   7,  10,  10,  99,\n",
       "         9,   4,  15,   7,  10,   5,   7, 191, 191,  10,   5, 191,  12,\n",
       "         2,  12,   1,   5,   7,   2,  10,  12,  99,   7, 191,   5,  15,\n",
       "         7, 191,  12,  12, 191,  15,  16,   7,   2,   7,  15,   7,  15,\n",
       "         7, 191,  99,   7,   7, 191,   5,   7,   5, 191, 191,  12, 191,\n",
       "        15,   5, 191,  15,  12,  15, 191,  12,  12, 191,  10,   7,  15,\n",
       "        15,   1,  15,   5,  12,  12,   5,  15,  15,   5,   6,  99,  15,\n",
       "         4,  12,  12,   5,   7,   7,   3, 191,   7,  15,   6,   7,  12,\n",
       "         6,   6,  10,   2,  15, 191,  15, 191,   7,  99,   7,  12,   6,\n",
       "         1,   6,   1, 191,   5, 191,  12,   6,  12,   7,   7, 191,   7,\n",
       "       191,   1,   7,  12,   7,  10,  12, 191,   5,   7, 191,  15,  12,\n",
       "        15, 191,   7,  15,   7,   5,   5,  12,  12,   4,   6,  10,   7,\n",
       "         5,  15,   2, 191,   7,   6, 191,  10,   5,   7,  15,   7, 191,\n",
       "         4,   6,  15,  12,  10,   7,   7,   7,  12,   5,   5,   5,   1,\n",
       "         4,  15,   7, 191,  15,   7,  12,  15,   2,   7,  12,   2,  15,\n",
       "       191,  12,   2,  15,   7,  15,  12,   6,   5,   7,  15,  15,   7,\n",
       "         7, 191,  12, 191,   7,   5,   7,  15,   5,   7,   7,  15,  15,\n",
       "        15,  15,   5,  10,  12,  12,  12,  12,  15,   7,   7, 191,   5,\n",
       "         5,  15,   5,  12,   2,  15,  15,   2,  12,   5,   5,   6,  15,\n",
       "       191, 191,   5, 191,   7,  12,  99,   7,   7,   6,   1,  10,  12,\n",
       "        12,   7,   5, 191,  12,   4,   7,   5, 191,  15,  99,   6,   6,\n",
       "       191,   5,   6,  15,  12,  12,  15,   5, 191, 191,   5,  15,  12,\n",
       "       191, 191, 191,   7,  15,   7,  15,   7,   5,   7,  12,  15,  15,\n",
       "         4, 191,   5,   7,  10,   5,   5,   7,   2,   5,   7, 191,  99,\n",
       "       191,   7,   7,   7,   4,   7,   2, 191,  12, 191, 191,   6,   5,\n",
       "         4])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NBmodSS.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3,  12,  16,  10,   5, 192,   6,   6,   4,  99,   2,   2,   9,\n",
       "        15, 192,   6,  10,   7,   1,  10,   1,   1,   5,  10, 191, 191,\n",
       "         1,  12,   7,  17,   6,   4,  12, 191,   1,  10,  15,   9, 191,\n",
       "        15,   4,   1,   1,  20,   7,   2,   2,  16,   6,   3,   4,   9,\n",
       "        12,  12,   7,   1,  10,   4,  10,  12,   9,   1,   1,   4,   6,\n",
       "        12,  15,  12,   9,  15,   4,   1,  10,   1, 191, 191,  99,   7,\n",
       "         7,   1,   3, 191,  10,  10,  10,  12,   2, 191,   1, 191,  17,\n",
       "         5,   3,  10,  12,  12, 191,  15,   7, 191, 192,   6, 192,  15,\n",
       "       191,  17,  12,   3,  20,  16,   4,  20,   9,   5,   7,   1,   9,\n",
       "       192,   5,  12,   4, 192,   7, 191,   2,  20,   4,  10,  12,  12,\n",
       "         1, 191,   6,   3,  12, 192,   7,  10,  10,  16, 191, 192,   1,\n",
       "        12, 191, 191, 192, 191, 191,  15,   6,  15,  15,  10,   3, 191,\n",
       "        15,  12,  15,   6,  15,  12,  20, 191,   3,  20,   5,  12,   7,\n",
       "         7,  17,  10,   6,  10,   9,   5,   4,   3,   3,   2,  16,   6,\n",
       "        16,   4,   3, 191,  10,  17, 192,   7,   5,   9,   5,   2,   4,\n",
       "       191, 191,   5,  12,   7, 191,   2,   4,  15,  15,   7,   9,  20,\n",
       "        10,   7, 191,   3, 192,  20, 192, 192,  10,   7,   6,   2,  99,\n",
       "         9,   4,  17,   7,  12,   5,   7, 191,  16,   6,   3,   2,   9,\n",
       "       191,   2,   1,   5,   7,  17,   3,   2,  15,   7, 191,  10,  15,\n",
       "         7, 191,  20,  12,  15,  15,  16,  20,   2,   4,   5,   4,  15,\n",
       "         7,  16,   1,  10,   7,  15,   5,  15,  10, 191, 191,  16,  16,\n",
       "        20,   3, 192, 192,   9,  15, 191,   9,  20, 191,  10,  99,   1,\n",
       "         1,  20,  17,   5,  12,   2,   5,   1,   1,   3,   6,  99,  15,\n",
       "        15,  16,  20,   5,   2,   4,   3, 191,  12, 192,  10,  15,   2,\n",
       "        10,  10,   2,   2,   9, 191,  12, 191,  17,  99,  15,  16,  16,\n",
       "         1,   3,  10,  16,   5, 191,   9,   6,   9,  17,   5,  16, 191,\n",
       "       191,   1,   7,   9,  15,  12,   2, 191,   5,   7, 191,  17,   9,\n",
       "         1,   9,   4, 192,  15,   1,  10,   9,   2,   4,   6,   9,   7,\n",
       "         2,  15,  16, 191,  16,   1, 191,   3,   5,  15,  10,  15, 191,\n",
       "        15,   6, 192,  12,   2,   4,   4,  20,  12,   3,   5,  10, 191,\n",
       "         4,  15,   7, 191,  17,   2,   2,   3,   3,  15,   3,  10,   9,\n",
       "        16,  12,   3,  15,   1, 192,  20,  20,  10,   7,  15, 192,  15,\n",
       "         7,  16,  20, 191,  15,   5,  17,   4,   5,  10,   7, 192, 192,\n",
       "       192,  12,  10,  10,  16,   9,   9,  12,  20,   7,  15, 191,   3,\n",
       "         1,  20,   3,  12,   2, 192, 192,   2,  12,   5,  10,   9,   9,\n",
       "        16,   7,   1,  15,   7,  15,  99,   3,   1,  99,   1,   9,  12,\n",
       "        16,   7,   3, 191,  12,   4,  10,   3,  16,   1,  99,   6,   1,\n",
       "       191,   5,  99, 192,   9,   9,   5,   1, 191, 191,  10,  12,  12,\n",
       "        16, 191, 191,   7,   1,  10,  17,  20,  10,  10,  12, 192,  15,\n",
       "        15,   7,  10,   7,  10,   3,   9,  20,   2,   9,   7,  16,   1,\n",
       "       191,  15,   4,  10,   4,  10,   2, 192,  20,   2,   2,   6,   3,\n",
       "         4])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import compress\n",
    "\n",
    "# Remove unlabeled\n",
    "X_train = list(compress(X_train, (y_train != -1))) \n",
    "y_train = y_train[(y_train != -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.ndarray.tolist(X_train)\n",
    "X_train = [item for sublist in X_train for item in sublist]\n",
    "X_test = np.ndarray.tolist(X_test)\n",
    "X_test = [item for sublist in X_test for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB # 1\n",
    "from sklearn.metrics import accuracy_score # Load sklearn tools\n",
    "\n",
    "# Load press release data (training and test)\n",
    "X_train = pd.read_csv(\"semi-files/X_train.csv\", index_col = 0).values\n",
    "y_train = np.asarray([int(i) for i in pd.read_csv(\"semi-files/y_train.csv\", \n",
    "index_col = 0).values])\n",
    "\n",
    "X_test = pd.read_csv(\"semi-files/X_test.csv\", index_col = 0).values\n",
    "y_test = np.asarray([int(i) for i in pd.read_csv(\"semi-files/y_test.csv\", \n",
    "index_col = 0).values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\FREELANCER\\\\SEMI_NB_TEXT_CLASSIFICATION'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-268-366cd81642fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0mcumulative_percent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;31m#set project directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'D:\\\\FREELANCER\\\\SEMI_NB_TEXT_CLASSIFICATION'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DATASET'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'stas_train.text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;31m#----------NaiveBayes with crossval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\FREELANCER\\\\SEMI_NB_TEXT_CLASSIFICATION'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Feb 19 04:51:16 2019\n",
    "\n",
    "@author: kennedy\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import re\n",
    "import codecs\n",
    "import os\n",
    "import numpy as np\n",
    "from mimetypes import guess_type\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class docparser(object):\n",
    "    \"\"\"\n",
    "    parser to get the Stsa dataset\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def transform_label_to_numeric(self, y):\n",
    "            if '1' in y:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "    def parse_line(self, row):\n",
    "        row = row.split(' ')\n",
    "        text = (' '.join(row[1:]))\n",
    "        label = self.transform_label_to_numeric(row[0])\n",
    "        return (re.sub(r'\\W+', ' ', text), label)\n",
    "\n",
    "    def get_data(self, file_path, Text = True):\n",
    "        if Text:\n",
    "          data = []\n",
    "          labels = []\n",
    "          f = codecs.open(file_path, 'r', encoding = \"utf8\",errors = 'ignore')\n",
    "          for line in f:\n",
    "              doc, label = self.parse_line(line)\n",
    "              data.append(doc)\n",
    "              labels.append(label)\n",
    "          return data, np.array(labels)\n",
    "        else:\n",
    "          import pandas as pd\n",
    "          df = pd.read_csv(file_path)\n",
    "          df.columns = ['Label', 'Rating', 'Review']\n",
    "          rating = pd.get_dummies(df.loc[:, 'Rating'])\n",
    "          text = []\n",
    "          labels = []\n",
    "          for ii, ij in zip(df.loc[:, 'Review'].values, df.loc[:, 'Label'].values):\n",
    "            #print(ii, ij)\n",
    "            if ii == ' ':\n",
    "              pass\n",
    "            else:\n",
    "              text.append(ii)\n",
    "              labels.append(ij)\n",
    "          return text, np.array(labels), rating\n",
    "  \n",
    "\n",
    "    def shuffle_dataset(self, X, y, seed=None):\n",
    "            \"\"\" Random shuffle of the samples \n",
    "            in X and y \n",
    "            \"\"\"\n",
    "            if seed:\n",
    "                np.random.seed(seed)\n",
    "            idx = [ii for ii in range(X.shape[0])]\n",
    "            np.random.shuffle(idx)\n",
    "            return X[idx], y[idx]\n",
    "      \n",
    "    def split(self, data, labels, test_size = 0.3,shuff = True, seed = None):\n",
    "        '''\n",
    "        :params\n",
    "          --label: labels of the dataset\n",
    "          --rating: rating as dummies categorical variables\n",
    "          --text: text data\n",
    "        :Returntype:\n",
    "           X_supervised:\n",
    "           X_unsupervised:\n",
    "           y_supervised:\n",
    "        '''\n",
    "        X = np.array(data)\n",
    "        y = np.array(labels)\n",
    "        if shuff:\n",
    "          X, y = self.shuffle_dataset(X, y, seed)\n",
    "        else:\n",
    "          split = len(y) - int(len(y) // (1/test_size))\n",
    "          X_supvsd, X_unsupvsd = X[:split], X[split:]\n",
    "          y_supvsd, y_supvsd = y[:split], y[split:]\n",
    "          return X_supvsd, X_unsupvsd, y_supvsd, y_supvsd\n",
    "      \n",
    " #%% Semisupervised NB Classifier\n",
    " \n",
    "class NaiveBayesSemiSupervised(object):\n",
    "    \"\"\"\n",
    "    This class implements a modification of the Naive Bayes classifier\n",
    "    in order to deal with unlabelled data. We use an Expectation-maximization \n",
    "    algorithm (EM). \n",
    "    This work is based on the paper\n",
    "    'Semi-Supervised Text Classification Using EM' by\n",
    "    Kamal Nigam Andrew McCallum Tom Mitchell\n",
    "    available here:\n",
    "    https://www.cs.cmu.edu/~tom/pubs/NigamEtAl-bookChapter.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, max_features=None, max_rounds=50, tolerance=1e-6):\n",
    "        \"\"\"\n",
    "        constructor for NaiveBayesSemiSupervised object\n",
    "        keyword arguments:\n",
    "            -- max_features: maximum number of features for documents vectorization\n",
    "            -- max_rounds: maximum number of iterations for EM algorithm\n",
    "            -- tolerance: threshold (in percentage) for total log-likelihood improvement during EM\n",
    "        \"\"\"\n",
    "        self.max_features = max_features\n",
    "        self.n_labels = 0\n",
    "        self.max_rounds = max_rounds\n",
    "        self.tolerance = tolerance\n",
    "          \n",
    "          \n",
    "    def train(self, X_supervised, X_unsupervised, y_supervised, y_unsupervised):\n",
    "        \"\"\"\n",
    "        train the modified Naive bayes classifier using both labelled and \n",
    "        unlabelled data. We use the CountVectorizer vectorizaton method from scikit-learn\n",
    "        positional arguments:\n",
    "            -- X_supervised: list of documents (string objects). these documents have labels\n",
    "                example: [\"all parrots are interesting\", \"some parrots are green\", \"some parrots can talk\"]\n",
    "            -- X_unsupervised: list of documents (string objects) as X_supervised, but without labels\n",
    "            -- y_supervised: labels of the X_supervised documents. list or numpy array of integers. \n",
    "                example: [2, 0, 1, 0, 1, ..., 0, 2]\n",
    "            -- X_supervised, X_unsupervised, y_supervised, y_unsupervised\n",
    "        \"\"\"\n",
    "        count_vec = CountVectorizer(max_features = self.max_features)\n",
    "        count_vec.fit(X_supervised)\n",
    "        self.n_labels = len(set(y_supervised))\n",
    "        if self.max_features is None:\n",
    "            self.max_features = len(count_vec.vocabulary_)\n",
    "        X_supervised = np.asarray(count_vec.transform(X_supervised).todense())\n",
    "        X_unsupervised = np.asarray(count_vec.transform(X_unsupervised).todense())\n",
    "        #train Naive Bayes\n",
    "        self.train_naive_bayes(X_supervised, y_supervised)\n",
    "        predi = self.predict(X_supervised)\n",
    "        old_likelihood = 1\n",
    "        final_accuracy = 0\n",
    "        while self.max_rounds > 0:\n",
    "            self.max_rounds -= 1\n",
    "            predi = self.predict(X_unsupervised)\n",
    "            self.train_naive_bayes(X_unsupervised, predi)\n",
    "            predi = self.predict(X_unsupervised)\n",
    "            correct = 0\n",
    "            for ij in predi:\n",
    "              if ij == 1:\n",
    "                correct += 1\n",
    "            correct_percent = correct/len(X_unsupervised)\n",
    "            if correct_percent > 0:\n",
    "              final_accuracy = correct_percent\n",
    "            print(str(correct_percent) + \"%\")\n",
    "            total_likelihood = self.get_log_likelihood( X_supervised, X_unsupervised, y_supervised)\n",
    "            print(\"total likelihood: {}\".format(total_likelihood))\n",
    "            if self._stopping_time(old_likelihood, total_likelihood):\n",
    "                print('Log likelihood not improved..Stopping EM at %s'%self.max_rounds)\n",
    "                break\n",
    "            old_likelihood = total_likelihood.copy()\n",
    "        return final_accuracy\n",
    "            \n",
    "    def _stopping_time(self, old_likelihood, new_likelihood):\n",
    "        \"\"\"\n",
    "        returns True if there is no significant improvement in log-likelihood and false else\n",
    "        positional arguments:\n",
    "            -- old_likelihood: log-likelihood for previous iteration\n",
    "            -- new_likelihood: new log-likelihood\n",
    "        \"\"\"\n",
    "        relative_change = np.absolute((new_likelihood-old_likelihood)/old_likelihood) \n",
    "        if (relative_change < self.tolerance):\n",
    "            print(\"stopping time\")\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "          \n",
    "    def get_log_likelihood(self, X_supervised, X_unsupervised, y_supervised):\n",
    "        \"\"\"\n",
    "        returns the total log-likelihood of the model, taking into account unsupervised data\n",
    "        positional arguments:\n",
    "            -- X_supervised: list of documents (string objects). these documents have labels\n",
    "                example: [\"all parrots are interesting\", \"some parrots are green\", \"some parrots can talk\"]\n",
    "            -- X_unsupervised: list of documents (string objects) as X_supervised, but without labels\n",
    "            -- y_supervised: labels of the X_supervised documents. list or numpy array of integers. \n",
    "                example: [2, 0, 1, 0, 1, ..., 0, 2]\n",
    "        \"\"\"\n",
    "        unsupervised_term = np.sum(self._predict_proba_unormalized(X_unsupervised), axis=1)\n",
    "        unsupervised_term = np.sum(np.log(unsupervised_term))\n",
    "        supervised_term = self._predict_proba_unormalized(X_supervised)\n",
    "        supervised_term = np.take(supervised_term, y_supervised)\n",
    "        supervised_term = np.sum(np.log(supervised_term))\n",
    "        total_likelihood = supervised_term + unsupervised_term\n",
    "        return total_likelihood\n",
    "\n",
    "    def word_proba(self, X, y, c):\n",
    "        \"\"\"\n",
    "        returns a numpy array of size max_features containing the conditional probability\n",
    "        of each word given the label c and the model parameters\n",
    "        positional arguments:\n",
    "            -- X: data matrix, 2-dimensional numpy ndarray\n",
    "            -- y: numpy array of labels, example: np.array([2, 0, 1, 0, 1, ..., 0, 2])\n",
    "            -- c: integer, the class upon which we condition\n",
    "        \"\"\"\n",
    "        numerator = 1 + np.sum( X[np.equal( y, c )], axis=0)\n",
    "        denominator = self.max_features + np.sum( X[ np.equal( y, c)])\n",
    "        return np.squeeze(numerator)/denominator\n",
    "\n",
    "    def class_proba(self, X, y, c):\n",
    "        \"\"\"\n",
    "        returns a numpy array of size n_labels containing the conditional probability\n",
    "        of each label given the label model parameters\n",
    "        positional arguments:\n",
    "            -- X: data matrix, 2-dimensional numpy ndarray\n",
    "            -- y: numpy array of labels, example: np.array([2, 0, 1, 0, 1, ..., 0, 2])\n",
    "            -- c: integer, the class upon which we condition\n",
    "        \"\"\"\n",
    "        numerator = 1 + np.sum( np.equal( y, c) , axis=0)\n",
    "        denominator = X.shape[0] + self.n_labels\n",
    "        return numerator/denominator\n",
    "\n",
    "    def train_naive_bayes(self, X, y):\n",
    "        \"\"\"\n",
    "        train a regular Naive Bayes classifier\n",
    "        positional arguments:\n",
    "             -- X: data matrix, 2-dimensional numpy ndarray\n",
    "             -- y: numpy array of labels, example: np.array([2, 0, 1, 0, 1, ..., 0, 2])\n",
    "        \"\"\"\n",
    "        word_proba_array = np.zeros((self.max_features, self.n_labels))\n",
    "        for c in range(self.n_labels):\n",
    "            word_proba_array[:,c] = self.word_proba( X, y, c)\n",
    "        labels_proba_array = np.zeros(self.n_labels)\n",
    "        for c in range(self.n_labels):\n",
    "            labels_proba_array[c] = self.class_proba( X, y, c)\n",
    "        self.word_proba_array = word_proba_array\n",
    "        self.labels_proba_array = labels_proba_array\n",
    "\n",
    "    def _predict_proba_unormalized(self, X_test):\n",
    "        \"\"\"\n",
    "        returns unormalized predicted probabilities (useful for log-likelihood computation)\n",
    "        positional arguments:\n",
    "             -- X: data matrix, 2-dimensional numpy ndarray\n",
    "        \"\"\"\n",
    "        proba_array_unormalized = np.zeros((X_test.shape[0], self.n_labels))\n",
    "        for c in range(self.n_labels):\n",
    "            temp = np.power(np.tile(self.word_proba_array[:,c], (X_test.shape[0] ,1)), X_test)\n",
    "            proba_array_unormalized[:,c] = self.labels_proba_array[c] * np.prod(temp, axis=1)\n",
    "        return proba_array_unormalized\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        returns model predictions (probability)\n",
    "        positional arguments:\n",
    "             -- X: data matrix, 2-dimensional numpy ndarray\n",
    "        \"\"\"\n",
    "        proba_array_unormalized = self._predict_proba_unormalized(X)\n",
    "        proba_array = np.true_divide(proba_array_unormalized, np.sum(proba_array_unormalized, axis=1)[:, np.newaxis])\n",
    "        return proba_array\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        returns model predictions (class labels)\n",
    "        positional arguments:\n",
    "             -- X: data matrix, 2-dimensional numpy ndarray\n",
    "        \"\"\"\n",
    "        return np.argmax(self.predict_proba( X), axis=1)\n",
    "      \n",
    "if __name__ == '__main__':\n",
    "    import os\n",
    "    import random\n",
    "    random.seed(23)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    #import naive_bayes\n",
    "    #import em\n",
    "    import nltk\n",
    "    #---------Global parameters-----------------\n",
    "    Text = False\n",
    "    max_features=None\n",
    "    NSPLIT = 4\n",
    "    n_sets = 10\n",
    "    set_size = 1.0 / n_sets\n",
    "    cumulative_percent = 0\n",
    "    #set project directory\n",
    "    os.chdir('D:\\\\FREELANCER\\\\SEMI_NB_TEXT_CLASSIFICATION')\n",
    "    file_path = os.path.join('DATASET','stas_train.text')\n",
    "    #----------NaiveBayes with crossval\n",
    "#    labeled_reviews = naive_bayes.get_labeled_reviews(\"D:\\\\FREELANCER\\\\SEMI_NB_TEXT_CLASSIFICATION\\\\DATASET\\\\mytracks_NaiveBayes_Filter.csv\")\n",
    "    if Text:\n",
    "      pass\n",
    "    else:\n",
    "      data, labels, rating = docparser().get_data(os.path.join('DATASET','mytracks_NaiveBayes_Filter.csv'), Text)\n",
    "      #----uncomment the line below to run both your NB and the semi-NB together\n",
    "#      labeled_reviews = naive_bayes.get_labeled_reviews(os.path.join('DATASET','mytracks_NaiveBayes_Filter.csv'))\n",
    "#      all_words = {}\n",
    "#      for (r, label) in labeled_reviews:\n",
    "#          for word in r.split(\" \"):\n",
    "#              if len(word) > 1:\n",
    "#                  all_words[word] = 0\n",
    "#    #featureset for NB\n",
    "#    featuresets = [(naive_bayes.review_features(r, all_words), label) for (r, label) in labeled_reviews]\n",
    "#    print('Start Naive Bayes Classification')\n",
    "#    naive_bayes.cross_validation(featuresets, n_sets)\n",
    "    print('End of Naive Bayes Classiation\\n')\n",
    "    print('*'*40)\n",
    "    print('Begin Semi NaiveBayes Classification')\n",
    "    #validation\n",
    "    _cum_acc = []\n",
    "    for i in range(0, n_sets):\n",
    "      n_training = int(set_size * len(data))\n",
    "      split_start = i * n_training\n",
    "      split_end = (i + 1) * n_training\n",
    "      train_data_before = data[:split_start]\n",
    "      train_data_after = data[split_end:]\n",
    "      X_supervised = train_data_before + train_data_after\n",
    "      X_unsupervised = data[split_start:split_end]\n",
    "      #for labels\n",
    "      train_labels_before = labels[:split_start]\n",
    "      train_labels_after = labels[split_end:]\n",
    "      y_supervised = list(train_labels_before) + list(train_labels_after)\n",
    "      y_unsupervised = labels[split_start:split_end]\n",
    "      #Max features should be left as it is 5738\n",
    "      clf = NaiveBayesSemiSupervised(max_features)\n",
    "      #train and evaluate accuracy\n",
    "      rst = clf.train(X_supervised, X_unsupervised, np.array(y_supervised), y_unsupervised)\n",
    "      _cum_acc.append(rst)\n",
    "    print('*'*40)\n",
    "    print('End of Semi-NaiveBayes')\n",
    "    print('Final Accuracy after {}fold Cross-validation is: {}'.format(n_sets, np.mean(np.array(_cum_acc))))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import nltk\n",
    "\n",
    "\n",
    "def get_labeled_reviews(path_to_csv):\n",
    "    labeled_reviews = []\n",
    "    with open(path_to_csv, newline='', encoding='utf-8') as csvfile:\n",
    "        review_reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        next(review_reader, None)  # Skip csv headers\n",
    "        for row in review_reader:\n",
    "            label = int(row[0])\n",
    "            review_text = row[2]\n",
    "\n",
    "            review = (review_text, label)\n",
    "            labeled_reviews.append(review)\n",
    "\n",
    "    return labeled_reviews\n",
    "\n",
    "\n",
    "def review_features(review, all_words):\n",
    "    #features = {}\n",
    "    features = all_words.copy()\n",
    "    #features[\"review\"] = review\n",
    "    for word in str.split(review, \" \"):\n",
    "        if len(word) > 1:\n",
    "            if word in features:\n",
    "                features[word] += 1\n",
    "            else:\n",
    "                features[word] = 1\n",
    "    return features\n",
    "\n",
    "def cross_validation(all_data, n_sets):\n",
    "    set_size = 1.0 / n_sets\n",
    "    shuffled_data = all_data.copy()\n",
    "    random.shuffle(shuffled_data)\n",
    "    cumulative_percent = 0\n",
    "    for i in range(0, 2):\n",
    "        n_training = int(set_size * len(all_data))\n",
    "        split_start = i * n_training\n",
    "        split_end = (i + 1) * n_training\n",
    "        print(\"train split_start: \" + str(split_start) + \" - split_end: \" + str(split_end))\n",
    "        train_data_before = shuffled_data[:split_start]\n",
    "        train_data_after = shuffled_data[split_end:]\n",
    "        train_data = train_data_before + train_data_after\n",
    "        test_data = shuffled_data[split_start:split_end]\n",
    "        print('{}\\n{}\\n{}'.format(train_data_before, train_data_after, train_data))\n",
    "        # print(\"train size: \" + str(len(train_data)) + \" - test size: \" + str(len(test_data)))\n",
    "        classifier = nltk.NaiveBayesClassifier.train(train_data, nltk.LaplaceProbDist)\n",
    "        correct = 0\n",
    "        for i, (t, l) in enumerate(test_data):\n",
    "            classified = classifier.classify(t)\n",
    "            # actual = labeled_reviews[split_point:][i][1]\n",
    "            if classified == l:\n",
    "                correct += 1\n",
    "        print(str(correct) + \"/\" + str(len(test_data)))\n",
    "        correct_percent = correct/len(test_data)\n",
    "        cumulative_percent += correct_percent\n",
    "        print(str(correct_percent) + \"%\")\n",
    "    print(\"Average result: \" + str(cumulative_percent / n_sets) + \"%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
