---
title: "Evaluation of textmodels"
author: "Cornelius Erfort"
date: "8/5/2021"
output: 
  pdf_document:
    dev: cairo_pdf
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, tidy.opts=list(width.cutoff = 80), tidy = T, python.reticulate = F, dev = "cairo_pdf")
knitr::opts_knit$set(root.dir = dirname(getwd()))
```



## Loading packages

This script is based mainly on the functions of the quanteda package. For the cross-validation of the textmodels, quanteda.classifiers has to be loaded from GitHub.

```{r packages, message=FALSE, warning=FALSE, results='hide'}
start_time <- Sys.time()

packages <- c(
  "quanteda", "quanteda.textmodels", "dplyr", "caret", "randomForest", "tm", "rmarkdown", "plyr", "readr", "ggplot2", "stringr", "formatR", "readstata13", "lubridate", "reticulate", "doMC", "glmnet", "kableExtra", "stargazer", "extrafont", "tidyr", "ggrepel", "irr")

lapply(packages[!(packages %in% rownames(installed.packages()))], install.packages)

if(!("quanteda.classifiers" %in% rownames(installed.packages()))) {
  remotes::install_github("quanteda/quanteda.classifiers")
} 

invisible(lapply(c(packages, "quanteda.classifiers"), require, character.only = T))

loadfonts()
loadfonts(device = "pdf")
theme_update(text = element_text(family = "LM Roman 10")) # Set font family for ggplot

if(!dir.exists("supervised-files")) dir.create("supervised-files")

source("scripts/functions.R")

load("files/press.RData")

```


# Load results from supervised models

```{r load}
load("files/tm_eval.RData")

```

# Add SuperLearner models

```{r transformer}
super_pred <- read.csv("files/superlearner-prediction.csv", header = F, col.names = c("country", "cv_sample", "weight", "issue", "prediction"))

super_eval <- super_pred %>% dplyr::group_by(country, cv_sample, weight) %>% dplyr::summarise(precision = quanteda.classifiers::precision(prediction, issue, by_class = T)) %>%  unnest_wider(precision) %>% pivot_longer(cols = unique(super_pred$issue) %>% as.character(), names_to = "issue", values_to = "precision")  %>% 
  merge(super_pred %>% dplyr::group_by(country, cv_sample, weight) %>% dplyr::summarise(accuracy = quanteda.classifiers::accuracy(prediction, issue, by_class = T)), by = c("country", "cv_sample", "weight"))
                                                                                
super_eval <- super_pred %>% dplyr::group_by(country, cv_sample, weight) %>% dplyr::summarise(recall = quanteda.classifiers::recall(prediction, issue, by_class = T)) %>%  unnest_wider(recall) %>% pivot_longer(cols = unique(super_pred$issue) %>% as.character(), names_to = "issue", values_to = "recall")  %>% merge(super_eval, by = c("country", "cv_sample", "issue", "weight"))

super_eval <- super_pred %>% dplyr::group_by(country, cv_sample, weight) %>% dplyr::summarise(f1 = quanteda.classifiers::f1_score(prediction, issue, by_class = T)) %>%  unnest_wider(f1) %>% pivot_longer(cols = unique(super_pred$issue) %>% as.character(), names_to = "issue", values_to = "f1")  %>% merge(super_eval, by = c("country", "cv_sample", "issue", "weight"))


super_times <- read.csv("files/superlearner-times.csv", header = F, col.names = c("time", "cv_sample", "weight", "country"))
super_times$time <- super_times$time/60

super_eval <- merge(super_eval, super_times, by = c("country", "cv_sample", "weight"))

super_eval$model_name <- "SuperLearner"
super_eval$model_id[super_eval$weight == "uniform"] <- 11
super_eval$model_id[super_eval$weight == "tfidf"] <- 12

super_eval
super_eval <- super_eval %>% dplyr::rename("k" = "cv_sample", "class" = "issue")

tm_eval <- rbind.fill(tm_eval, super_eval)



```


# Add Transformer models (mono- and multilingual)

```{r transformer}
# load("files/tm_eval.RData")
# tm_eval <- filter(tm_eval, model_id < 11)

# Monolingual
mono_pred <- read.csv("files/monolingual-prediction.csv", header = F, col.names = c("country", "cv_sample", "label", "prediction", "id"))

mono_eval <- mono_pred %>% dplyr::group_by(country, cv_sample) %>% dplyr::summarise(precision = quanteda.classifiers::precision(prediction, label, by_class = T)) %>%  unnest_wider(precision) %>% pivot_longer(cols = c(0:22) %>% as.character(), names_to = "label", values_to = "precision")  %>% 
  merge(mono_pred %>% dplyr::group_by(country, cv_sample) %>% dplyr::summarise(accuracy = quanteda.classifiers::accuracy(prediction, label, by_class = T)), by = c("country", "cv_sample"))
                                                                                
mono_eval <- mono_pred %>% dplyr::group_by(country, cv_sample) %>% dplyr::summarise(recall = quanteda.classifiers::recall(prediction, label, by_class = T)) %>%  unnest_wider(recall) %>% pivot_longer(cols = c(0:22) %>% as.character(), names_to = "label", values_to = "recall")  %>% merge(mono_eval, by = c("country", "cv_sample", "label"))

mono_eval <- mono_pred %>% dplyr::group_by(country, cv_sample) %>% dplyr::summarise(f1 = quanteda.classifiers::f1_score(prediction, label, by_class = T)) %>%  unnest_wider(f1) %>% pivot_longer(cols = c(0:22) %>% as.character(), names_to = "label", values_to = "f1")  %>% merge(mono_eval, by = c("country", "cv_sample", "label"))

mono_eval <- merge(mono_eval, data.frame(issue = unique(press$issue) %>% sort, label = (1:length(unique(press$issue) %>% sort) - 1)), by = "label") %>% select(-c(label))

mono_times <- read.csv("files/monolingual-times.csv", header = F, col.names = c("time", "cv_sample", "country"))
mono_times$time <- mono_times$time/60

mono_eval <- merge(mono_eval, mono_times, by = c("country", "cv_sample"))

mono_eval$model_name <- "Monolingual Transformer"
mono_eval$model_id <- 13

mono_eval <- mono_eval %>% dplyr::rename("k" = "cv_sample", "class" = "issue")

tm_eval <- rbind.fill(tm_eval, mono_eval)

# Multilingual
multi_pred <- read.csv("files/multilingual-prediction.csv", header = F, col.names = c("country", "cv_sample", "label", "prediction", "id"))

# multi_eval <- multi_pred %>% dplyr::group_by(country, cv_sample) %>% dplyr::summarise(accuracy_issue = quanteda.classifiers::accuracy(prediction, label, by_class = T)) %>%  unnest_wider(accuracy_issue) %>% pivot_longer(cols = c(0:22) %>% as.character(), names_to = "label", values_to = "precision")  %>%
  # merge(multi_pred %>% dplyr::group_by(country, cv_sample) %>% dplyr::summarise(accuracy_issue = quanteda.classifiers::accuracy(prediction, label, by_class = T)), by = c("country", "cv_sample"))
                                                    

multi_eval <- multi_pred %>% 
  dplyr::group_by(country, cv_sample) %>% 
  dplyr::summarise(precision = quanteda.classifiers::precision(prediction, label, by_class = T)) %>%  
  unnest_wider(precision) %>% pivot_longer(cols = c(0:22) %>% as.character(), names_to = "label", values_to = "precision")  %>%
  merge(multi_pred %>% dplyr::group_by(country, cv_sample) %>% dplyr::summarise(accuracy = quanteda.classifiers::accuracy(prediction, label, by_class = T)), by = c("country", "cv_sample"))
                        

multi_pred %>% dplyr::group_by(country, cv_sample) %>% dplyr::summarise(accuracy = quanteda.classifiers::accuracy(prediction, label, by_class = T))
                                                        
multi_eval <- multi_pred %>% dplyr::group_by(country, cv_sample) %>% 
  dplyr::summarise(recall = quanteda.classifiers::recall(prediction, label, by_class = T)) %>%  
  unnest_wider(recall) %>% 
  pivot_longer(cols = c(0:22) %>% as.character(), names_to = "label", values_to = "recall")  %>%
  merge(multi_eval, by = c("country", "cv_sample", "label"))

multi_eval <- multi_pred %>% 
  dplyr::group_by(country, cv_sample) %>% 
  dplyr::summarise(f1 = quanteda.classifiers::f1_score(prediction, label, by_class = T)) %>%  
  unnest_wider(f1) %>% pivot_longer(cols = c(0:22) %>% as.character(), names_to = "label", values_to = "f1")  %>% 
  merge(multi_eval, by = c("country", "cv_sample", "label"))


class_acc <- function(prediction, label) {
 val <- sapply(unique(label), FUN = function(i)(sum(prediction[label == i] == i) + sum(prediction[label != i] != i)) / length(label))
 names(val) <- unique(label)
 list(val)
}

multi_eval <- multi_pred %>% 
  dplyr::group_by(country, cv_sample) %>% 
  dplyr::summarise(class_acc = class_acc(prediction, label))  %>%  
  unnest_wider(class_acc) %>% pivot_longer(cols = c(0:22) %>% as.character(), names_to = "label", values_to = "class_acc")  %>% 
  merge(multi_eval, by = c("country", "cv_sample", "label"))

multi_eval <- merge(multi_eval, data.frame(issue = unique(press$issue) %>% sort, label = (1:length(unique(press$issue) %>% sort) - 1)), by = "label") %>% select(-c(label))

multi_times <- read.csv("files/multilingual-times.csv", header = F, col.names = "time")/60
multi_times$cv_sample <- 1:5

multi_eval <- merge(multi_eval, multi_times, by = "cv_sample")

multi_eval$model_name <- "Multilingual Transformer"
multi_eval$model_id <- 14

multi_eval <- multi_eval %>% dplyr::rename("k" = "cv_sample", "class" = "issue")

tm_eval <- rbind.fill(tm_eval, multi_eval)




tm_eval$accuracy <- tm_eval$accuracy %>% unlist()

```


# Evaluate text models
```{r tm-eval}

table(tm_eval$country, tm_eval$model_id)
tm_eval$model_id %>% unique %>% sort

tm_eval <- tm_eval %>% mutate(alpha = ifelse(is.na(alpha), "", alpha),
                              distribution = ifelse(is.na(distribution), "", distribution),
                              type = ifelse(is.na(type), "", type),
                              smooth = ifelse(is.na(smooth), "", smooth),
                              seed = ifelse(is.na(seed), "", seed),
                              weight = ifelse(is.na(weight), "", weight),)

tm_eval$accuracy <- tm_eval$accuracy %>% unlist

tm_eval_mean <- aggregate(cbind(accuracy, precision, recall, f1) ~ country + model_name + model_id + weight  + distribution + type + smooth + alpha, tm_eval, function (x) round(mean(x, na.rm = T)*100,2))
tm_eval_mean <- tm_eval_mean[order(tm_eval_mean$country, -tm_eval_mean$accuracy), ]

# Summary across countries
# mutate(time = ifelse((model_id <= 8) & time > 2, time/60, time),) %>% 
# time = round(time, 2)
tm_eval_out <- tm_eval_mean %>%  
  dplyr::group_by(country, model_name, model_id, weight, distribution, type, smooth, alpha) %>% 
  top_n(1, accuracy) %>% 
  aggregate(cbind(accuracy, precision, recall, f1) ~ model_name + model_id + weight  + distribution + type + smooth + alpha, ., function (x) round(mean(x, na.rm = T),2)) %>% 
  group_by(model_name) %>% 
  top_n(1, accuracy) %>% 
  select(model_name, accuracy, precision, recall, f1) %>% dplyr::rename("Model name" = model_name, "Accuracy (\\%)" = accuracy, "Precision (\\%)" = precision, "Recall (\\%)" = recall, "F1 score (\\%)" = f1) %>% as.data.frame() #, "Time (min)" = time)

latex_out <- capture.output(tm_eval_out %>%
  stargazer(out = "tables/tm-eval.tex", summary = F, rownames = F, digits = 2,
            title = "Summary of classifier performance", label = "tab:tm-eval"))

# Top 3 per country summary
# mutate(time = ifelse((model_id <= 8) & time > 2, time/60, time),) %>% 
#, time = round(time, 2)
tm_eval_out <- tm_eval_mean %>%  
  group_by(country, model_name) %>% 
  top_n(1, accuracy) %>% 
  aggregate(cbind(accuracy, precision, recall, f1) ~ country + model_name, ., function (x) mean(x, na.rm = T)) %>% 
  group_by(country) %>% 
  top_n(3, accuracy) %>% 
  select(country, model_name, accuracy, precision, recall, f1) %>% dplyr::rename(Country = country, "Model name" = model_name, "Accuracy (\\%)" = accuracy, "Precision (\\%)" = precision, "Recall (\\%)" = recall, "F1 score (\\%)" = f1) %>% 
  mutate(Country = str_to_title(Country) %>% str_replace("Uk", "UK")) %>% as.data.frame()

tm_eval_out <- tm_eval_out[order(tm_eval_out$Country, -tm_eval_out[, 3]), ]

latex_out <- capture.output(tm_eval_out %>%
  stargazer(digits = 2, out = "tables/tm-eval-country.tex", summary = F, rownames = F, 
            title = "Best three classifier by country", label = "tab:tm-eval-country"))


```



```{r tables}
categories <- data.frame(issue = c(1:10, 12:18, 20, 23, 98, 99, 19.1, 19.2), 
             title = c("Macroeconomics", "Civil Rights", "Health", "Agriculture", "Labor", "Education", "Environment", "Energy", "Immigration", "Transportation", "Law and Crime", "Social Welfare","Housing", "Domestic Commerce", "Defense", "Technology", "Foreign Trade", "Government Operations", "Culture", "Non-thematic", "Other", "International Affairs", "European Integration"))

freq <- table(((press %>% filter(cv_sample > 0))$country), ((press %>% filter(cv_sample > 0))$issue)) %>% as.data.frame() %>% dplyr::rename("country" = "Var1", "issue" = "Var2", "n_labeled_issue" = "Freq") %>% mutate(issue = str_replace_all(issue, c("191" = "19.1", "192" = "19.2")) %>% as.numeric) %>% merge((press %>% filter(cv_sample > 0))$country %>% table %>% as.data.frame() %>% dplyr::rename("country" = ".", "n_labeled" = "Freq"), by = "country")

tm_eval_issue <- tm_eval %>% mutate(class_acc = ifelse(is.na(class_acc), NaN, class_acc), 
                                    precision = ifelse(is.na(precision), NaN, precision), 
       recall = ifelse(is.na(recall), NaN, recall), 
       f1 = ifelse(is.na(f1), NaN, f1), ) %>% 
  filter(model_name == "Multilingual Transformer") %>% mutate(class_acc = ifelse(is.nan(class_acc), -99999, class_acc),
                                                              precision = ifelse(is.nan(precision), -99999, precision),
                                                             recall = ifelse(is.nan(recall), -99999, recall),
                                                             f1 = ifelse(is.nan(f1), -99999, f1),
                                                             accuracy = unlist(accuracy)) %>%
  aggregate(cbind(class_acc, precision, recall, f1) ~ country + class + model_name, ., function (x) mean(x, na.rm = T)) %>% 
  dplyr::rename(issue = class) %>% 
  mutate(issue = str_replace_all(issue, c("191" = "19.1", "192" = "19.2")) %>% 
           str_remove("\\.0") %>% factor(levels = c(1:10, 12:18, 19.1, 19.2, 20, 23, 98, 99))) %>% 
  merge(freq, by = c("country", "issue")) %>% 
  merge(categories, by = "issue") %>% 
  mutate(class_acc = ifelse(class_acc < 0, NaN, class_acc), 
         precision = ifelse(precision < 0, NaN, precision),
                                                             recall = ifelse(recall  < 0, NaN, recall),
                                                             f1 = ifelse(f1 < 0, NaN, f1)) %>% 
  select(c(country, issue, title, class_acc, precision, recall, f1, n_labeled_issue, n_labeled)) %>% 
  dplyr::rename(Country = country, Issue = issue, Accuracy = class_acc,  Precision = precision, Recall = recall, "F1 score" = f1, Title = title, "n Issue" = n_labeled_issue, "n Country" = n_labeled) %>% # Accuracy = accuracy,
  mutate(Accuracy = round(Accuracy*100, 2),
         Precision = round(Precision*100, 2),
         Recall = round(Recall*100, 2),
         `F1 score` = round(`F1 score`*100, 2),
         Country = str_to_title(Country) %>% str_replace("Uk", "UK"))

tm_eval_issue <- tm_eval_issue[order(tm_eval_issue$Country, tm_eval_issue$Issue), ] 

table(tm_eval_issue$Issue, tm_eval_issue$Country)

cross <- filter(press, !is.na(issue) & !is.na(issue_cross)) %>% select("country", "issue", "issue_cross")

cross$issue <- str_replace_all(cross$issue, c("191" = "19.1", "192" = "19.2")) %>% factor(levels = c(1:10, 12:18, 19.1, 19.2, 20, 23, 98, 99))
cross$issue_cross <- str_replace_all(cross$issue_cross, c("191" = "19.1", "192" = "19.2")) %>% factor(levels = c(1:10, 12:18, 19.1, 19.2, 20, 23, 98, 99))

# Performance by category (for each country, for best model)
for (country in unique(tm_eval_issue$Country)) {
  print(country)
    latex_out <- capture.output(filter(tm_eval_issue, Country == country) %>% select(-c(Country)) %>% 
                                  cbind(Intercoder = f1_score(cross$issue[cross$country == tolower(country)], cross$issue_cross[cross$country == tolower(country)])) %>% mutate(f1 = 100*f1) %>%
                                  dplyr::rename(Intercoder = f1) %>% select(c(Issue, Title, Accuracy, Precision, Recall, "F1 score", Intercoder, "n Issue")) %>% 
                                  stargazer(out = str_c("tables/tm-eval-", country %>% str_to_lower(),".tex"), summary = F, rownames = F,  digits = 1, notes = unique(tm_eval_issue$`n Country`[tm_eval_issue$Country == country]) %>% as.character(), 
                                            title = str_c("Classifier performance by issue, ", country), label = str_c("tab:tm-eval-", country %>% str_to_lower())))
}


# Performance by model type (for each country)
# for (country in unique(tm_eval_issue$Country)) {
#   
#     latex_out <- capture.output(this_table %>%
#   stargazer(out = str_c("tables/tm-eval-", country %>% str_to_lower(),".tex"), summary = F, rownames = F, digits = 1,
#             title = str_c("Summary of classifier performance, ", country), label = str_c("tab:tm-eval-", country %>% str_to_lower())))
# }


# F1 score for each country and category in one table


select(tm_eval_issue, c(Country, Issue, Title, "F1 score")) %>% 
  pivot_wider(names_from = "Country", values_from = "F1 score") %>% 
  mutate(Issue = str_c(Issue, " - ", Title)) %>% select(-c(Title)) %>% 
  cbind(
Average = (select(tm_eval_issue, c(Country, Issue, Title, "F1 score")) %>% 
  pivot_wider(names_from = "Country", values_from = "F1 score") %>% 
  mutate(Issue = str_c(Issue, " - ", Title)) %>% select(-c(Title)))[, -c(1)] %>% rowMeans(na.rm = T)) %>% 
  cbind(Intercoder = f1_score(cross$issue, cross$issue_cross)) %>% mutate(f1 = 100*f1) %>% dplyr::rename(Intercoder = f1) %>% 
  stargazer(summary = F,rownames = F, 
            title = "Issue-specific performance: F1 score by country", 
            label = "f1-country",
            out = "tables/f1-country.tex", digits = 1)


```

# Tables with number of press releases per country (year x party)
```{r coverage}

table(press$country)

press$year <- factor(press$year)

for (country in unique(press$country)) table(str_c(press$party_name[press$country == country], " (", press$parlgov_id[press$country == country], ")"), press$year[press$country == country]) %>% unclass %>% stargazer(summary = F, rownames = F, 
            title = str_c("Number of press releases per party, ", str_to_title(country) %>% str_replace("Uk" ,"UK")), 
            label = str_c("tab:coverage-", country),
            out = str_c("tables/coverage-", country, ".tex"))

```


# Confusion matrix

For best supervised model Ridge (predicted) against human codings (truth).

```{r  confusion-ridge}
# Baseline model (highest cross-validated accuracy)
if(!file.exists("files/ridge_pred.RData")) {
  ridge_pred <- data.frame()

presscorpus <- corpus(str_c(press$header, " ", press$text),
                       docvars = select(press, c(country, id, issue, party_name, cv_sample)))

for (country in press$country %>% unique) {
  print(country)
  
  countrycorpus <-  presscorpus[presscorpus$country == country & presscorpus$cv_sample != -1, ]
  ndoc(countrycorpus) %>% print
  
  # Stopwords
  if(country == "poland") countrystop <-  stopwords::stopwords("pl", source = "stopwords-iso") else countrystop <- stopwords(str_replace_all(country, c("germany" = "de", "austria" = "de", "spain" = "es", "netherlands" = "nl", "ireland" = "en", "uk" = "en", "sweden" = "sw", "denmark" = "da")))
  countrystop

  # Create dfm
  dfmat <- countrycorpus %>%
    dfm(remove = countrystop, # Stem and remove stopwords, punctuation etc.
        stem = T, remove_punct = T, remove_number = T, remove_symbols = T, remove_url = T) %>% 
    dfm_trim(min_docfreq = 0.005, max_docfreq = .9, # Remove words occurring <.5% or > 80% of docs
             docfreq_ = "prop") %>%
    suppressWarnings()
  
  # Create alternative dfm (bigrams and tfidf)
  dfmat_alt <- countrycorpus %>%
    tokens() %>% tokens_ngrams(n = 1:2) %>%
    dfm(remove = countrystop, # Stem and remove stopwords, punctuation etc.
        stem = T, remove_punct = T, remove_number = T, remove_symbols = T, remove_url = T) %>% 
    dfm_trim(max_docfreq = .06, # Remove words occurring >6% of docs
             docfreq_ = "prop") %>%
    dfm_trim(min_docfreq = 5, # Remove words occurring in <5 docs
             docfreq_ = "count") %>% suppressWarnings()

  for (i in 1:5) {
    print(i)
    ridge_pred <- data.frame(country = country,
                             cv_sample = i,
                             prediction = textmodel_svm(dfm_subset(dfmat_alt, dfmat_alt$cv_sample != i), dfm_subset(dfmat_alt, dfmat_alt$cv_sample != i)$issue, type = 7) %>%
        predict(., newdata = dfm_subset(dfmat_alt, dfmat_alt$cv_sample == i)),
        issue = dfm_subset(dfmat_alt, dfmat_alt$cv_sample == i)$issue) %>% rbind.fill(ridge_pred)
    
  }
}

ridge_pred$issue <- str_replace_all(ridge_pred$issue, c("191" = "19.1", "192" = "19.2")) %>% factor(levels = c(1:10, 12:18, 19.1, 19.2, 20, 23, 98, 99))
ridge_pred$prediction <- str_replace_all(ridge_pred$prediction, c("191" = "19.1", "192" = "19.2")) %>% factor(levels = c(1:10, 12:18, 19.1, 19.2, 20, 23, 98, 99))

save(ridge_pred, file = "files/ridge-pred.RData")
} else load("files/ridge-pred.RData")


confusion_abs <- (table(ridge_pred$prediction, ridge_pred$issue) %>%
  confusionMatrix(mode = "sens_spec", dnn = c("predicted", "truth")))$table %>% 
  cbind(., rowSums(.)) %>% rbind(., colSums(.))


# Normalized (rowsums)
confusion <- round((table(ridge_pred$prediction, ridge_pred$issue) %>%
  confusionMatrix(mode = "sens_spec", dnn = c("predicted", "truth")))$table / ((table(ridge_pred$prediction, ridge_pred$issue) %>%
  confusionMatrix(mode = "sens_spec", dnn = c("predicted", "truth")))$table %>% rowSums()) * 100, 0)

# confusion <- cbind(confusion, (table(ridge_pred$prediction)))
confusion <- cbind(confusion, confusion_abs[, ncol(confusion_abs)]) %>% rbind(confusion_abs[nrow(confusion_abs),] )

latex_out <- capture.output(as.data.frame(unclass(confusion))
 %>%
  stargazer(summary = F,
            title = "Confusion matrix for the test data (Ridge (L2), normalized)", 
            label = "tab:confusion-ridge"))

latex_out <- capture.output(latex_out %>% str_replace_all( "tabular", "tabularx")  %>% str_replace_all("\\@\\{\\\\extracolsep\\{5pt\\}\\} ccccccccccccccccccccccccc", "\\\\textwidth\\}\\{XXXXXXXXXXXXXXXXXXXXXXXXX") %>% cat(sep = "\n"), file = "tables/confusion-ridge.tex")

```

```{r  confusion-transformer}

multi_pred <- read_csv("files/multilingual-prediction.csv", col_names = c("country", "cv_sample", "label", "prediction", "id")) %>% dplyr::rename("pred_label" = "prediction") %>% 
  merge(., data.frame(issue = unique(press$issue) %>% sort, label = (1:length(unique(press$issue) %>% sort) - 1)), by = "label") %>% select(-c(label)) %>% 
  merge(., data.frame(prediction = unique(press$issue) %>% sort, pred_label = (1:length(unique(press$issue) %>% sort) - 1)), by = "pred_label") %>% select(-c(pred_label))

# Confusion matrix and overall statistics
multi_pred$issue %>% table

multi_pred$issue <- str_replace_all(multi_pred$issue, c("191" = "19.1", "192" = "19.2")) %>% factor(levels = c(1:10, 12:18, 19.1, 19.2, 20, 23, 98, 99))
multi_pred$prediction <- str_replace_all(multi_pred$prediction, c("191" = "19.1", "192" = "19.2")) %>% factor(levels = c(1:10, 12:18, 19.1, 19.2, 20, 23, 98, 99))

if(!file.exists("files/multi_pred.RData")) save(multi_pred, file = "files/multi_pred.RData")

confusion_abs <- (table(multi_pred$prediction, multi_pred$issue) %>%
  confusionMatrix(mode = "sens_spec", dnn = c("predicted", "truth")))$table %>% 
  cbind(., rowSums(.)) %>% rbind(., colSums(.))

# Normalized (rowsums)
confusion <- round((table(multi_pred$prediction, multi_pred$issue) %>%
  confusionMatrix(mode = "sens_spec", dnn = c("predicted", "truth")))$table / ((table(multi_pred$prediction, ridge_pred$issue) %>%
  confusionMatrix(mode = "sens_spec", dnn = c("predicted", "truth")))$table %>% rowSums()) * 100, 0)

confusion <- cbind(confusion, confusion_abs[, ncol(confusion_abs)]) %>% rbind(confusion_abs[nrow(confusion_abs),] )

latex_out <- capture.output(as.data.frame(unclass(confusion))
 %>%
  stargazer(summary = F,
            title = "Confusion matrix for the test data (Multilingual Transformer, normalized)", 
            label = "tab:confusion-tf"))

latex_out <- capture.output(latex_out %>% str_replace_all( "tabular", "tabularx")  %>% str_replace_all("\\@\\{\\\\extracolsep\\{5pt\\}\\} cccccccccccccccccccccccccc", "\\\\textwidth\\}\\{r|XXXXXXXXXXXXXXXXXXXXXXXXX") %>% cat(sep = "\n"), file = "tables/confusion-tf.tex")

```

```{r  cross}
cross <- filter(press, !is.na(issue) & !is.na(issue_cross)) %>% select("country", "issue", "issue_cross")

# Confusion matrix and overall statistics
press$issue %>% table
press$issue_cross %>% table

cross$issue <- str_replace_all(cross$issue, c("191" = "19.1", "192" = "19.2")) %>% factor(levels = c(1:10, 12:18, 19.1, 19.2, 20, 23, 98, 99))
cross$issue_cross <- str_replace_all(cross$issue_cross, c("191" = "19.1", "192" = "19.2")) %>% factor(levels = c(1:10, 12:18, 19.1, 19.2, 20, 23, 98, 99))

save(cross, file = "files/cross.RData")

confusion_abs <- (table(cross$issue_cross, cross$issue) %>%
  confusionMatrix(mode = "sens_spec", dnn = c("predicted", "truth")))$table %>% 
  cbind(., rowSums(.)) %>% rbind(., colSums(.))

# Normalized (rowsums)
confusion <- round((table(cross$issue_cross, cross$issue) %>%
  confusionMatrix(mode = "sens_spec", dnn = c("predicted", "truth")))$table / ((table(cross$issue_cross, cross$issue) %>%
  confusionMatrix(mode = "sens_spec", dnn = c("predicted", "truth")))$table %>% rowSums()) * 100, 0)

confusion <- cbind(confusion, confusion_abs[, ncol(confusion_abs)]) %>% rbind(confusion_abs[nrow(confusion_abs),] )

latex_out <- capture.output(as.data.frame(unclass(confusion))
 %>%
  stargazer(summary = F,
            title = "Confusion matrix for hand coding (Coder 1 x Coder 2))", 
            label = "tab:confusion-hand"))

latex_out <- capture.output(latex_out %>% str_replace_all( "tabular", "tabularx")  %>% str_replace_all("\\@\\{\\\\extracolsep\\{5pt\\}\\} ccccccccccccccccccccccccc", "\\\\textwidth\\}\\{r|XXXXXXXXXXXXXXXXXXXXXXXX") %>% cat(sep = "\n"), file = "tables/confusion-hand.tex")


# Krippendorff's alpha
load("files/cross.RData")

cross %>%  dplyr::group_by(country) %>% 
  dplyr::summarise(kalpha = kripp.alpha(as.matrix(cbind(issue, issue_cross)) %>% t, method = "nominal")['value'] %>% unlist) %>% as.data.frame() %>% mutate(country = str_to_title(country) %>% str_replace("Uk", "UK")) %>% 
  stargazer(summary = F,rownames = F, 
            title = "Inter-coder agreement, Krippendorff's alpha", 
            label = "krippendorf",
            out = "tables/krippendorff.tex", digits = 3)




```


```{r script_eval}
# Time needed to run script (much shorter when textmodels are just loaded from a file)
# The estimation time for the single textmodels can found in the table above.

print(Sys.time() - start_time) 

# In total, the script needs about 2-3h to run.
