---
title: "ReadMe"
author: "Cornelius Erfort"
date: "5/24/2021"
output: 
  pdf_document:
    dev: cairo_pdf
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, tidy.opts=list(width.cutoff = 80), tidy = T, python.reticulate = F)
knitr::opts_knit$set(root.dir = dirname(getwd()))
```


# Setting up

This script requires the files which are not included on GitHub.

## Loading packages


```{r packages, message=FALSE, warning=FALSE, results='hide'}
start_time <- Sys.time()

packages <- c("dplyr", "tm", "rmarkdown", "plyr", "readr", "ggplot2", "stringr", "formatR", "readstata13", "lubridate", "extrafont", "kableExtra", "stargazer", "word2vec", "tokenizers", "ggforce")

lapply(packages[!(packages %in% rownames(installed.packages()))], install.packages)

if(!("readme" %in% rownames(installed.packages()))) {
  remotes::install_github("iqss-research/readme-software/readme")
} 

if(!("tensorflow" %in% rownames(installed.packages()))) {
  remotes::install_github("rstudio/tensorflow")
} 

invisible(lapply(c(packages, "readme", "tensorflow"), require, character.only = T))

loadfonts()
loadfonts(device = "pdf")
theme_update(text = element_text(family = "LM Roman 10")) # Set font family for ggplot

source("scripts/functions.R")

set.seed(1621447882) # Set a seed if you choose

if(!dir.exists("readme-files")) dir.create("readme-files")

```

## Loading the corpus

The sample data for Germany consists of 2,612 labeled press releases. The dataset is not uploaded on GitHub.

The corpus is generated in the script "Preparing the textual data". We create a text corpus based on the header and text of each press release.

```{r data, out.width = "80%"}
load("readme-files/alldocs.RData")
table(alldocs$issue == -1)
issue_categories <- 
  data.frame(issue = c(1:10, 12:18, 20, 23, 98, 99, 191:192), 
             issue_descr = c("Macroeconomics", "Civil Rights", 
                                "Health", "Agriculture", "Labor", "Education", "Environment", "Energy", 
                                "Immigration", "Transportation", "Law and Crime", "Social Welfare",
                             "Housing", "Domestic Commerce", "Defense", "Technology", 
                             "Foreign Trade", "Government Operations", "Culture", "Non-thematic", "Other", "International Affairs", "European Integration"))
```

# Readme2: Calculating proportions

In this section we use the package readme2 by Jerzak et al. (2019) to estimate the proportion of press releases regarding each topic. We do so by defining five folds for cross-validation. Our test dataset thus makes up 20% of documents.

In a first step, we load a word vector (embeddings trained on German Wikipedia, source: https://deepset.ai/german-word-embeddings). Second, we generate word vector summaries for all documents. Third, we define five folds of our training dataset. Fourth, we run the readme function to obtain predictions about the proportions in our test data.

(Jerzak, C. T., King, G., & Strezhnev, A. (forthcoming). An improved method of auto-mated nonparametric content analysis for social science. Political Analysis.)

```{r readme}
# Run readme for each fold
if(!("readme_cv" %in% ls())) if(file.exists("readme-files/readme_cv.RData")) { # Load when file exists
  
  load("readme-files/readme_cv.RData") } else { # If not load word vector and calculate summaries
    
if(!("wordVec_summaries_all" %in% ls())) if(file.exists("readme-files/wordvectors/wordVec_summaries_all.RData")) {
  
  load ("readme-files/wordvectors/wordVec_summaries_all.RData") } else {
    
    # Read raw vector data and create vector matrix 
    # (https://deepset.ai/german-word-embeddings)
    if(!("glove_ger" %in% ls())) if(file.exists("readme-files/wordvectors/glove_ger.RData")) {
      load ("readme-files/wordvectors/glove_ger.RData") } else {
      glove_ger_raw <- scan(file = "readme-files/wordvectors/GloVe-german.txt", what="", sep="\n")
      glove_ger <- proc_pretrained_vec(glove_ger_raw)
      glove_feat <- names(glove_ger)
      glove_ger <- glove_ger %>% t() %>% as.matrix()
      save(glove_ger, file = "readme-files/wordvectors/glove_ger.RData")
    
      }
    
  ## Generate a word vector summary for all documents
    
    # Clear env
    rm(list = ls()[!(ls() %in% c("alldocs", "glove_ger", "seed", "issue_categories"))]) 
    gc()
    memory.limit()
    memory.size()
    
    wordVec_summaries_all <- undergrad(
    documentText = alldocs[alldocs$issue != -1, ],
    wordVecs = glove_ger)
    
    save(wordVec_summaries_all, file = "readme-files/wordvectors/wordVec_summaries_all.RData")
  }

    
  readme_cv <- list()
  readme_time <- list()
  
for (i in unique(alldocs$cv_sample[alldocs$issue != -1]) %>% sort) {
  # Estimate category proportions
  start_readme <- Sys.time()
  readme_cv[[i]] <- readme(
    dfm = wordVec_summaries_all[alldocs$issue != -1, ], 
    labeledIndicator = ifelse(alldocs$cv_sample[alldocs$issue != -1] == i, 0, 1),
    categoryVec = alldocs$issue[alldocs$issue != -1],
    nCores = 16,
    nCores_OnJob = 8) %>% suppressWarnings()
  readme_time[[i]] <- Sys.time() - start_readme
}
  save(readme_cv, file = "readme-files/readme_cv.RData")
}


```

# Evaluation of Readme2

In order to evaluate the performance of readme2, we compare the predicted proportions in the test data with the true values. We present a table and plot to illustrate the link between predicted and true values.

``` {r readme_agg}
# Prediction estimate and truth in %
readme_agg <- data.frame()
for (i in 1:length(readme_cv)) {
  readme.estimates <- readme_cv[[i]]
  readme.estimates$point_readme
  
  readme_agg <- data.frame(
    issue = attr(readme.estimates$point_readme, "names"), 
    predicted = readme.estimates$point_readme %>% as.vector(),
    truth = (table(alldocs$issue[alldocs$cv_sample == i])/sum(table(alldocs$issue[alldocs$cv_sample == i]))) %>% as.vector(),
    cv_sample = i
) %>% rbind.fill(readme_agg)
  
}

# Difference in percentage points (positive values indicate an inflated prediction, i.e. we estimate a higher share for the category compared to the truth)
readme_agg$difference <- readme_agg$predicted - readme_agg$truth

readme_agg <- dplyr::mutate(readme_agg, predicted = round(predicted, 3), truth = round(truth, 3), difference = round(difference, 3))

# Change and order labels
readme_agg$issue[readme_agg$issue == 191] <- 19.1
readme_agg$issue[readme_agg$issue == 192] <- 19.2
readme_agg$issue <- as.factor(as.numeric(readme_agg$issue))
levels(readme_agg$issue) <- str_c(levels(readme_agg$issue), " - ", issue_categories[c(1:17, 22:23, 18:21), 2])

# Write latex table
if(!dir.exists("tables")) dir.create("tables")
latex_out <- capture.output(stargazer(readme_agg %>% dplyr::group_by(issue) %>%
  dplyr::summarise(predicted = mean (predicted),
            truth = mean(truth)) %>%
  dplyr::rename(issue = issue) %>% as.data.frame() %>%
  stargazer(summary = F, rownames = F,
            title = "Evaluation of aggregate values (readme2)",
            label = "tab:readme_agg_readme",
            notes = "Mean values from five-fold cross-validation."),
  out = "tables/readme_agg_readme.tex"))

# Plot aggregate evaluation
if(!dir.exists("plots")) dir.create("plots")

plot_agg_eval(readme_agg %>% dplyr::group_by(issue) %>% 
  dplyr::summarise(predicted = mean (predicted),
            truth = mean(truth)), "readme")

readme_agg <- readme_agg %>% dplyr::rename(Issue = issue, prediction = predicted) %>% select(-c(cv_sample, difference)) %>% dplyr::group_by(Issue) %>% dplyr::mutate(prediction = mean(prediction), truth = mean(truth), model = "Readme") %>% unique()

save(readme_agg, file = "readme-files/readme_agg.RData")

```


# Readme2 applied

Now we turn to estimating proportions for the unlabeled documents. We do so by running the software for a subset of press releases from each quarter and party. All labeled documents are included in each estimation.

This allows us, to compare the values with those obtained from the "classify and count" method using supervised text classification.

``` {r applied, message=FALSE, warning=FALSE, results='hide'}
if(!dir.exists("readme-files/readme-quarters")) dir.create("readme-files/readme-quarters")

# Loading full dataset (not on GitHub)
load("readme-files/alldocs.RData")
# all_germany <- read_rds("data/data_joint.RDS") %>% select(c(header, text.x, date.x, issue, party.x, id)) %>% filter(!is.na(date.x) & !is.na(text.x) & (issue != "98 Non-thematic" | is.na(issue))) %>% dplyr::rename(party = party.x, date = date.x, text = text.x)

nrow(alldocs)

# Unite parties
alldocs$party <- alldocs$party %>% str_replace_all(c("union_fraktion" = "CDU/CSU", "spd_fraktion" = "SPD", "90gruene_fraktion" = "B'90/Die Gr√ºnen", "fdp_bundesverband" = "FDP", "fdp_fraktion" = "FDP", "linke_fraktion" = "DIE LINKE", "afd_bundesverband" = "AfD", "afd_fraktion" = "AfD"))
table(alldocs$party)

# Merge categories
# alldocs$issue <- alldocs$issue %>% as.character %>% str_replace_all(c("a" = "1", "b" = "2")) %>% str_extract("[:digit:]*") %>% as.numeric

# Make quarterly date
alldocs$date <- as.character(alldocs$date) %>% substr(1, 8) %>% str_c("15") %>% str_replace_all(c("-01-" = "-02-", "-03-" = "-02-", "-04-" = "-05-", "-06-" = "-05-", "-07-" = "-08-", "-09-" = "-08-", "-10-" = "-11-", "-12-" = "-11-")) %>%  ymd()

# Duplicate labeled as unlabeled (because otherwise they are not used for the estimated proportions)

load ("readme-files/wordvectors/wordVec_summaries_all.RData") 
nrow(wordVec_summaries_all)

wordVec_summaries_all <- rbind(wordVec_summaries_all, wordVec_summaries_all[alldocs$issue != -1, ])
nrow(wordVec_summaries_all)

alldocs <- rbind(alldocs, filter(alldocs, issue != -1) %>% mutate(issue = -1))
nrow(alldocs)
table(alldocs$issue == -1)
table(alldocs$cv_sample)

# Go through parties
issue_agendas_readme <- data.frame()
for (party in unique(alldocs$party)) {
  print(party)
  
  # Go through quarters
  for (date in unique(alldocs$date)) {
    print(date %>% as.Date(origin = "1970-01-01"))
    
    if(sum((alldocs$party == party & alldocs$date == date)) == 0) next # Only   continue if there are documents for party/time
    
    if(file.exists(str_c("readme-files/readme-quarters/", party %>% str_replace_all("/", "-"), "_", date %>%   as.Date(origin = "1970-01-01"), ".RData"))) { # Load if file exists
      load(str_c("readme-files/readme-quarters/", party %>% str_replace_all("/", "-"), "_", date %>% as.Date(origin =   "1970-01-01"), ".RData"))
    } else {
      
      
    # Subset corpus to party/time and run readme
    
    docs_quarter <- wordVec_summaries_all[(alldocs$party == party & alldocs$date == date) | alldocs$issue != -1, ] # All docs belonging to party/quarter plus all labeled docs
    vars_quarter <-  alldocs[(alldocs$party == party & alldocs$date == date) | alldocs$issue != -1, ] # All docs belonging to party/quarter plus all labeled docs
    
    readme_storage <- readme(
      dfm = docs_quarter,
      labeledIndicator =  ifelse(vars_quarter$issue != -1, 1, 0),
      categoryVec = ifelse(vars_quarter$issue == -1, NA, vars_quarter$issue),
      nCores = 16,
      nCores_OnJob = 8) # %>% suppressWarnings()
    
    # Save output to file
    save(readme_storage, file = str_c("readme-files/readme-quarters/", party %>% str_replace_all("/", "-"), "_", date %>%   as.Date(origin = "1970-01-01"), ".RData"))
    }
    
    issue_agendas_readme <- data.frame(
      issue = attr(readme_storage$point_readme, "names"),
      attention = readme_storage$point_readme %>% as.vector(),
      date = date  %>% as.Date(origin = "1970-01-01"),
      party = party) %>% rbind.fill(issue_agendas_readme)
    
  }
}

issue_agendas_readme <- merge(issue_agendas_readme, issue_categories, by = "issue")
save(issue_agendas_readme, file = "readme-files/issue_agendas_readme.RData")

```


```{r script_eval}
# Time needed to run script (much shorter when results are just loaded from a file)
print(Sys.time() - start_time) 

# Running readme for all parties/quarters took about 24h
```
```