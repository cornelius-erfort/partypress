---
title: "Application to unlabeled data"
author: "Cornelius Erfort"
date: "8/5/2021"
output: 
  pdf_document:
    dev: cairo_pdf
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, tidy.opts=list(width.cutoff = 80), tidy = T, python.reticulate = F)
knitr::opts_knit$set(root.dir = dirname(getwd()))
```

# Summary

We predict issue labels for all unlabeled German press releases and calculate the share of press releases dedicated to each issue area for each quarter.

# Setting up

This script requires the files which are not included on GitHub.

At the end of this script, the file "issue_agendas.RData" is saved. It contains quarterly estimates for the share of press releases for each issue and party.

## Loading packages

```{r packages, message=FALSE, warning=FALSE, results='hide'}
start_time <- Sys.time()

packages <- c(
  "quanteda", "quanteda.textmodels", "dplyr", "caret", "randomForest", "tm", "rmarkdown", "plyr", "readr", "ggplot2", "stringr", "formatR", "readstata13", "lubridate", "reticulate", "doMC", "glmnet", "kableExtra", "stargazer", "extrafont", "ggrepel")

lapply(packages[!(packages %in% rownames(installed.packages()))], install.packages)

if(!("quanteda.classifiers" %in% rownames(installed.packages()))) {
  remotes::install_github("quanteda/quanteda.classifiers")
} 

invisible(lapply(c(packages, "quanteda.classifiers"), require, character.only = T))

loadfonts()
loadfonts(device = "pdf")
theme_update(text = element_text(family = "LM Roman 10")) # Set font family for ggplot

source("scripts/functions.R")

load("files/press.RData")

```

# Classification of unlabeled data

## Using the fine-tuned Transformers

We trained the models using a set of 2,612 labeled documents. In order to obtain aggregated measures of issue attention, we predict the issue categories of all ? labeled and unlabeled press releases in our sample.

```{r ridge-predict}

presscorpus <- corpus(str_c(press$header, " ", press$text),
                       docvars = select(press, c(country, id, issue, party_name, cv_sample)))

if(!file.exists("files/ridge_pred_all.RData")) {
  ridge_pred_all <- data.frame()
for (country in press$country %>% unique) {
  print(country)
  
  # if(country %in% unique(ridge_pred_all$country)) next
  
  countrycorpus <-  presscorpus[presscorpus$country == country, ]
  ndoc(countrycorpus) %>% print
  
  # Stopwords
  if(country == "poland") countrystop <-  stopwords::stopwords("pl", source = "stopwords-iso") else countrystop <- stopwords(str_replace_all(country, c("germany" = "de", "austria" = "de", "spain" = "es", "netherlands" = "nl", "ireland" = "en", "uk" = "en", "sweden" = "sw", "denmark" = "da")))
  countrystop
  
  # Create alternative dfm (bigrams and tfidf)
  dfmat_alt <- countrycorpus[countrycorpus$cv_sample != -1, ] %>%
    tokens() %>% tokens_ngrams(n = 1:2) %>%
    dfm(remove = countrystop, # Stem and remove stopwords, punctuation etc.
        stem = T, remove_punct = T, remove_number = T, remove_symbols = T, remove_url = T) %>% 
    dfm_trim(max_docfreq = .06, # Remove words occurring >6% of docs
             docfreq_ = "prop") %>%
    dfm_trim(min_docfreq = 5, # Remove words occurring in <5 docs
             docfreq_ = "count") %>% suppressWarnings()
  
  dfm_all <- countrycorpus %>%
    tokens() %>% tokens_ngrams(n = 1:2) %>%
    dfm(remove = countrystop, # Stem and remove stopwords, punctuation etc.
        stem = T, remove_punct = T, remove_number = T, remove_symbols = T, remove_url = T) %>% 
    dfm_match(dfmat_alt@Dimnames$features)

    ridge_pred_all <- data.frame(country = country,
                             id = dfm_all$id,
                             prediction = textmodel_svm(dfm_subset(dfm_all, dfm_all$cv_sample != -1), dfm_subset(dfm_all, dfm_all$cv_sample != -1)$issue, type = 7) %>%
        predict(., newdata = dfm_all)) %>% rbind.fill(ridge_pred_all)
    # save(ridge_pred_all, file = "files/ridge_pred_all.RData")
} else load("files/ridge_pred_all.RData")

    
}
```


```{r unlabeled}

# Load the predicted labels
multi_pred_all <- read_csv("files/multilingual-all_predictions.csv", col_names = F)
names(multi_pred_all) <- c("label", "id", "country")
multi_pred_all <- merge(multi_pred_all, data.frame(multi = unique(press$issue) %>% sort, label = (1:length(unique(press$issue) %>% sort) - 1)), by = "label", all.x = T) %>% select(-c(label))
press <- merge(press, multi_pred_all, by = c("country", "id"))


ridge_pred_all <- dplyr::rename(ridge_pred_all, ridge = prediction)
press <- merge(press, ridge_pred_all, by = c("country", "id"))

press <- filter(press, party_name != "Kukiz'15 (2015-2019)")
press$party_name %>% table

# Tables for samples of press releases
# Immigration
sample9 <- select(press, c("country", "party_name", "date", "header", "multi")) %>% filter(multi == 9)
(sample9 <- sample9[sample(1:nrow(sample9), 20), ])

sample9 %>% dplyr::rename(party = party_name, title = header) %>% select(-c(multi))


latex_out <- capture.output(sample9 %>% dplyr::rename(party = party_name, title = header) %>% select(-c(multi)) %>%
  stargazer(type = "latex", summary = F, rownames = F, 
            title = "Sample of press releases classified as category 9 - Immigration", label = "tab:9-document-samples"))

latex_out <- capture.output(latex_out %>% str_replace_all( "tabular", "tabularx")  %>% str_replace_all("\\@\\{\\\\extracolsep\\{5pt\\}\\} ccc", "\\\\textwidth\\}\\{stX") %>% cat(sep = "\n"), file = "tables/9-document-samples.tex")
```


## Aggregation of the issues categories over time and party

To measure parties' evolving issue agendas, we aggregate the category counts over time.

```{r aggregation}

# Create dataframe with only necessary vars
issue_agendas <- press %>% select(c(country, date, multi, party_name)) %>% dplyr::rename(party = party_name) %>% dplyr::rename(issue = multi)

# Make date quarterly
issue_agendas$date <- as.character(issue_agendas$date) %>% substr(1, 8) %>% str_c("15") %>% str_replace_all(c("-01-" = "-02-", "-03-" = "-02-", "-04-" = "-05-", "-06-" = "-05-", "-07-" = "-08-", "-09-" = "-08-", "-10-" = "-11-", "-12-" = "-11-")) %>%  ymd()

# Add variable for counting
issue_agendas$freq <- 1

# Aggregate by party, date and issue
issue_agendas <- aggregate(freq ~ country + party + date + issue, issue_agendas, sum)

# Add observations with zero documents
for (thisparty in unique(issue_agendas$party)) {
  for(thisdate in unique(issue_agendas$date[issue_agendas$party == thisparty])) {
    for(thisissue in unique(issue_agendas$issue)) {
      if(nrow(issue_agendas[issue_agendas$party == thisparty & issue_agendas$date == thisdate & issue_agendas$issue == thisissue, ]) == 0 & nrow(issue_agendas[issue_agendas$party == thisparty & issue_agendas$date == thisdate, ]) != 0) {
        issue_agendas <- data.frame(
          party = thisparty,date = thisdate, issue = thisissue, freq = 0
          ) %>% rbind.fill(issue_agendas)
}}}}

# Add var for total press releases per party and month
issue_agendas$party_sum <- ave(issue_agendas$freq, issue_agendas$date, issue_agendas$party, FUN = sum)

issue_agendas$attention <- issue_agendas$freq / issue_agendas$party_sum

issue_agendas$issue %>% table

# Add issue descriptions

issue_categories <- 
  data.frame(issue = c(1:10, 12:18, 191:192, 20, 23, 98, 99), 
             issue_descr = c("Macroeconomics", "Civil Rights", "Health", "Agriculture", "Labor", "Education", 
              "Environment", "Energy", "Immigration", "Transportation", "Law and Crime", 
              "Social Welfare", "Housing", "Domestic Commerce", "Defense", "Technology", 
              "Foreign Trade", "International Affairs", "European Integration", "Government Operations", "Culture", "Non-thematic", "Other"))

issue_agendas <- merge(issue_agendas, issue_categories, by = "issue") %>% select(-c(freq))

issue_agendas$date <- issue_agendas$date %>% as.Date(origin = "1970-01-01")

save(issue_agendas, file = "data/issue_agendas.RData")


```

# Visualize issue agendas

```{r}

# Environment and Energy
for (this_country in unique(issue_agendas$country) %>% sort) {

  plot_data <- filter(issue_agendas, country == this_country & issue == 9)
  
  plot_data <- filter(plot_data, party_sum > 5)
  
  thisplot <- ggplot(plot_data, aes(x = date, y = attention)) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          text = element_text(size = 16)) +
    scale_x_date(date_breaks = "2 years", date_labels = "%Y", limits = c(plot_data$date %>% min, plot_data$date %>% max)
) +
    ylab("Share of press releases per quarter")
  
  thisplot <- thisplot +
        geom_step(color = "dark grey", alpha = .8) +
        #geom_smooth(method = "loess", formula = "y ~ x", color = "dark grey", lty = 2, se = F, alpha = .3)
        geom_line(stat="smooth", method = "loess", formula = "y ~ x",
                size = .7,
                linetype ="dashed",
                alpha = 0.8, color = "black",
                se = F)
  
  if(min(plot_data$attention) >= 0) thisplot <- thisplot + ylim(c(0, NA))
  

    thisplot <- thisplot +
      geom_vline(xintercept = ymd("2015-08-31"), color = "dark grey", lty = 2)
  
  thisplot <- thisplot + facet_wrap(~ party)
  
  thisplot
  
    ggsave(str_c("plots/immigration-agenda-", this_country,".pdf"), device = cairo_pdf, width = 5*2^.5, height = 5)

}

```





```{r script_eval}
# Time needed to run script
print(Sys.time() - start_time) 

